<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Principal Component Analysis (PCA) Example</title>
    <!-- Include necessary CSS or styling if needed -->
    <link href="../estilos.css" rel="stylesheet" type="text/css">
</head>
<body class="texto8_1">

<h1>Principal Component Analysis (PCA) Example</h1>

<p>This is a simple example of Principal Component Analysis (PCA) using Python and scikit-learn.</p>

<h2>Principal Component Analysis Overview</h2>

<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional representation while retaining as much of the original variance as possible. PCA achieves this by identifying and preserving the principal components, which are orthogonal axes in the feature space corresponding to the directions of maximum variance.</p>

<p>Key concepts of Principal Component Analysis:</p>

<ul>
    <li><b>Variance:</b> The amount of spread or dispersion in the data.</li>
    <li><b>Principal Components:</b> The new axes obtained through linear combinations of the original features, ordered by the amount of variance they capture.</li>
    <li><b>Explained Variance:</b> The proportion of the total variance explained by each principal component.</li>
    <li><b>Dimensionality Reduction:</b> Projecting data onto a lower-dimensional subspace defined by a subset of principal components.</li>
</ul>

<p>PCA is widely used for visualization, noise reduction, and speeding up machine learning algorithms by reducing the number of features while preserving the most important information in the data.</p>

<p>Python Source Code:</p>

<pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.decomposition import PCA

# Generate synthetic data
np.random.seed(42)
X, _ = make_blobs(n_samples=100, n_features=2, centers=3, random_state=42)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Plot the original and PCA-transformed data
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c='blue', marker='o', label='Original Data')
plt.title('Original Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c='red', marker='o', label='PCA-Transformed Data')
plt.title('PCA-Transformed Data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()

plt.tight_layout()
plt.show()
</code></pre>

<p>Explanation:</p>

<ul>
    <li><b>Import Libraries:</b> Import necessary Python libraries, including NumPy for numerical operations, Matplotlib for plotting, and scikit-learn for dataset generation and PCA.</li>
    <li><b>Generate Synthetic Data:</b> Create synthetic data with three clusters using the <code>make_blobs</code> function from scikit-learn.</li>
    <li><b>Apply PCA:</b> Use scikit-learn's <code>PCA</code> to perform dimensionality reduction and obtain the principal components.</li>
    <li><b>Plot Results:</b> Visualize the original data and the PCA-transformed data in a side-by-side comparison.</li>
</ul>

<!-- Include necessary JavaScript libraries if needed -->

</body>
</html>
