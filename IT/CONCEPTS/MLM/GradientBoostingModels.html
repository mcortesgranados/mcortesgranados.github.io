<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Gradient Boosting Example</title>
    <!-- Include necessary CSS or styling if needed -->
    <link href="../estilos.css" rel="stylesheet" type="text/css">
</head>
<body class="texto8_1">

<h1>Gradient Boosting Example</h1>

<p>This is a simple example of Gradient Boosting using Python and scikit-learn.</p>

<h2>Gradient Boosting Overview</h2>

<p>Gradient Boosting is an ensemble learning technique that builds a predictive model by combining the strengths of multiple weak models, typically decision trees. The model is built sequentially, with each new tree attempting to correct the errors of the combined ensemble so far. Gradient Boosting is powerful and robust, often achieving high performance across various tasks.</p>

<p>Key concepts of Gradient Boosting:</p>

<ul>
    <li><b>Weak Learners:</b> The individual models, often shallow decision trees, that are combined to form a strong predictive model.</li>
    <li><b>Residuals:</b> The errors or differences between the predicted and actual values that subsequent models aim to minimize.</li>
    <li><b>Learning Rate:</b> A hyperparameter that controls the contribution of each weak learner to the overall model.</li>
    <li><b>Ensemble Building:</b> The iterative process of adding weak learners to the ensemble.</li>
    <li><b>Regularization:</b> Techniques like shrinkage and tree pruning to prevent overfitting.</li>
</ul>

<p>Gradient Boosting is widely used for regression and classification tasks and is implemented in libraries like scikit-learn, XGBoost, and LightGBM.</p>

<p>Python Source Code:</p>

<pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Gradient Boosting model
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train.ravel())

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Plot the results
plt.scatter(X_test, y_test, color='black')
plt.scatter(X_test, y_pred, color='red', marker='x')
plt.title('Gradient Boosting Example')
plt.xlabel('X')
plt.ylabel('y')
plt.show()
</code></pre>

<p>Explanation:</p>

<ul>
    <li><b>Import Libraries:</b> Import necessary Python libraries, including NumPy for numerical operations, Matplotlib for plotting, and scikit-learn for machine learning.</li>
    <li><b>Generate Synthetic Data:</b> Create synthetic data with a linear relationship and some random noise (similar to the Linear Regression example).</li>
    <li><b>Split Data:</b> Split the data into training and testing sets using the <code>train_test_split</code> function.</li>
    <li><b>Train Model:</b> Create and train a Gradient Boosting model using scikit-learn's <code>GradientBoostingRegressor</code>.</li>
    <li><b>Make Predictions:</b> Use the trained Gradient Boosting model to make predictions on the test set.</li>
    <li><b>Evaluate Model:</b> Calculate Mean Squared Error to evaluate the performance of the model.</li>
    <li><b>Plot Results:</b> Plot the actual vs. predicted values for visualization.</li>
</ul>

<!-- Include necessary JavaScript libraries if needed -->

</body>
</html>
