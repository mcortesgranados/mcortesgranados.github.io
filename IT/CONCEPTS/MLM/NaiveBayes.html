<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Naive Bayes Example</title>
    <!-- Include necessary CSS or styling if needed -->
    <link href="../estilos.css" rel="stylesheet" type="text/css">
</head>
<body class="texto8_1">

<h1>Naive Bayes Example</h1>

<p>This is a simple example of Naive Bayes using Python and scikit-learn.</p>

<h2>Naive Bayes Overview</h2>

<p>Naive Bayes is a probabilistic machine learning algorithm commonly used for classification tasks. It is based on Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event. The "naive" assumption in Naive Bayes is that features are conditionally independent given the class label, which simplifies the modeling process.</p>

<p>Key concepts of Naive Bayes:</p>

<ul>
    <li><b>Bayes' Theorem:</b> A mathematical formula for calculating conditional probabilities.</li>
    <li><b>Prior Probability:</b> The probability of a class before considering the evidence from features.</li>
    <li><b>Likelihood:</b> The probability of observing the features given a particular class.</li>
    <li><b>Posterior Probability:</b> The updated probability of a class after considering the evidence.</li>
    <li><b>Conditional Independence:</b> The assumption that features are independent given the class label (naive assumption).</li>
</ul>

<p>Naive Bayes is computationally efficient and works well in practice, especially for text classification and spam filtering. It is sensitive to the assumption of feature independence and may not perform well if this assumption is violated.</p>

<p>Python Source Code:</p>

<pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = (4 + 3 * X + np.random.randn(100, 1)) > 6  # Binary classification task

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train.ravel())

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# Plot the results (for binary classification)
plt.scatter(X_test, y_test, color='black', label='Actual')
plt.scatter(X_test, y_pred, color='red', marker='x', label='Predicted')
plt.title('Naive Bayes Example')
plt.xlabel('X')
plt.ylabel('Class (0 or 1)')
plt.legend()
plt.show()
</code></pre>

<p>Explanation:</p>

<ul>
    <li><b>Import Libraries:</b> Import necessary Python libraries, including NumPy for numerical operations, Matplotlib for plotting, and scikit-learn for machine learning.</li>
    <li><b>Generate Synthetic Data:</b> Create synthetic binary classification data with a linear relationship and some random noise.</li>
    <li><b>Split Data:</b> Split the data into training and testing sets using the <code>train_test_split</code> function.</li>
    <li><b>Train Model:</b> Create and train a Naive Bayes model using scikit-learn's <code>GaussianNB</code>.</li>
    <li><b>Make Predictions:</b> Use the trained Naive Bayes model to make predictions on the test set.</li>
    <li><b>Evaluate Model:</b> Calculate accuracy to evaluate the performance of the model (for binary classification).</li>
    <li><b>Plot Results:</b> Plot the actual vs. predicted values for visualization (for binary classification).</li>
</ul>

<!-- Include necessary JavaScript libraries if needed -->

</body>
</html>
