<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stochastic Gradient Descent (SGD) Example</title>
    <!-- Include necessary CSS or styling if needed -->
</head>
<body>

<h1>Stochastic Gradient Descent (SGD) Example</h1>

<p>This is a simple example of Stochastic Gradient Descent (SGD) using Python and the scikit-learn library.</p>

<h2>SGD Overview</h2>

<p>Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used for training machine learning models, especially in the context of large datasets. Unlike traditional gradient descent, which computes the gradient using the entire dataset, SGD updates the model parameters using a small random subset of the data (mini-batch) at each iteration. This makes it computationally more efficient and suitable for online learning.</p>

<p>Key concepts of SGD:</p>

<ul>
    <li><b>Gradient Descent:</b> An iterative optimization algorithm for finding the minimum of a function (minimizing a loss function).</li>
    <li><b>Stochastic Updates:</b> Model parameters are updated using a random subset (mini-batch) of the training data.</li>
    <li><b>Learning Rate:</b> Controls the step size in the parameter space during optimization.</li>
    <li><b>Iterations:</b> The number of passes through the entire dataset (epochs).</li>
</ul>

<p>SGD is widely used in training various machine learning models, including linear models, neural networks, and support vector machines.</p>

<p>Python Source Code:</p>

<pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SGDRegressor model
sgd_model = SGDRegressor(max_iter=100, eta0=0.01, random_state=42)
sgd_model.fit(X_train, y_train.ravel())

# Predict on the test set
y_pred = sgd_model.predict(X_test)

# Plot the model's predictions
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='black', label='True Data Points')
plt.plot(X_test, y_pred, color='red', label='SGDRegressor Prediction')
plt.title('Stochastic Gradient Descent (SGD) Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
</code></pre>

<p>Explanation:</p>

<ul>
    <li><b>Import Libraries:</b> Import necessary Python libraries, including NumPy for numerical operations, Matplotlib for plotting, and scikit-learn for SGDRegressor.</li>
    <li><b>Generate Synthetic Data:</b> Generate synthetic data for demonstration purposes.</li>
    <li><b>Split Data:</b> Split the data into training and testing sets.</li>
    <li><b>Train SGDRegressor Model:</b> Train the SGDRegressor model with specified hyperparameters (max_iter and eta0).</li>
    <li><b>Predict and Plot:</b> Predict on the test set and plot the model's predictions.</li>
</ul>

<!-- Include necessary JavaScript libraries if needed -->

</body>
</html>
