1. What is RabbitMQ and how does it differ from Kafka?
RabbitMQ is a message broker that facilitates communication between different systems by sending messages in queues. It uses the Advanced Message Queuing Protocol (AMQP). Kafka, on the other hand, is a distributed streaming platform designed to handle high throughput and store streams of records in partitions.

2. How do message brokers like RabbitMQ help decouple application components?
Message brokers allow components to communicate with each other asynchronously, meaning that the sender and receiver do not need to be aware of each other's existence or operate simultaneously. This decoupling improves scalability, reliability, and fault tolerance.

3. What is the concept of "queue" in RabbitMQ?
A queue in RabbitMQ is a buffer that stores messages until they are consumed by a consumer. Producers publish messages to a queue, and consumers fetch messages from the queue. Queues can be configured with different properties such as durability and message TTL (Time to Live).

4. How does Kafka ensure message durability?
Kafka ensures message durability by persisting messages to disk and replicating them across multiple brokers. Even if a broker fails, messages are still available due to the replication mechanism. Kafka provides configurable retention policies to control how long messages are stored.

5. What are the differences between ActiveMQ and RabbitMQ in terms of message delivery?
ActiveMQ is a message broker that supports both point-to-point (queues) and publish-subscribe (topics) messaging patterns. RabbitMQ primarily uses queues and supports AMQP for routing messages. While ActiveMQ has more complex routing features like message selectors, RabbitMQ is optimized for high concurrency and supports flexible routing based on exchange types.

6. How does message persistence work in ActiveMQ?
ActiveMQ persists messages by storing them in a persistent storage (like a database or file system). It guarantees message delivery even if the broker crashes. ActiveMQ supports both persistent and non-persistent messages, with persistent messages being stored in a journal.

7. What is a topic in ActiveMQ, and how is it different from a queue?
A topic in ActiveMQ is a form of the publish-subscribe model, where messages are broadcast to all subscribers. In contrast, a queue is a point-to-point model, where each message is consumed by only one subscriber. Topics allow multiple consumers to receive the same message simultaneously.

8. How does Kafka's partitioning model enhance scalability?
Kafka divides each topic into multiple partitions, allowing data to be distributed across multiple brokers. Each partition can be processed in parallel, enhancing throughput and scalability. Partitioning also allows for distributed storage and processing of large datasets.

9. What is the role of a consumer group in Kafka?
A consumer group in Kafka is a collection of consumers that work together to process messages from a topic. Each consumer in the group processes messages from different partitions, ensuring that each message is processed only once within the group, enabling parallel consumption and load balancing.

10. How do you configure message acknowledgment in RabbitMQ?
In RabbitMQ, message acknowledgment can be configured using automatic or manual acknowledgment. With manual acknowledgment, the consumer sends an acknowledgment to the broker after successfully processing the message, ensuring that messages are not lost in case of failures.

11. What is a producer in RabbitMQ, and how does it interact with queues?
A producer in RabbitMQ is an application or service that publishes messages to a queue. The producer connects to the broker and sends messages to a specified queue, from which consumers can later retrieve the messages.

12. What are the advantages of using Kafka for event-driven architecture?
Kafka is ideal for event-driven architectures due to its high throughput, fault tolerance, and scalability. It allows multiple producers and consumers to efficiently handle large volumes of events, providing strong durability and low-latency messaging.

13. Can RabbitMQ handle high throughput, and how does it achieve that?
Yes, RabbitMQ can handle high throughput by utilizing multiple queues and efficient message routing mechanisms. It can also be scaled horizontally by clustering multiple RabbitMQ nodes, balancing the load across them to improve throughput.

14. What are the use cases for using ActiveMQ's virtual destinations?
ActiveMQ's virtual destinations are used when multiple consumers need to consume from the same queue or topic without having duplicate messages. Virtual destinations allow the distribution of messages to different consumers while maintaining message integrity and order.

15. How does Kafka handle message retention?
Kafka handles message retention by allowing messages to be stored in topics for a configurable period of time or until the size of the topic exceeds a certain threshold. This ensures that messages are available for consumers to read at any time within the retention period.

16. How do you configure durable queues in RabbitMQ?
To configure durable queues in RabbitMQ, you must set the queue's `durable` property to `true`. This ensures that the queue survives broker restarts and retains its messages as long as they are not acknowledged or expired.

17. What is the difference between "at least once" and "exactly once" delivery semantics in Kafka?
"At least once" delivery semantics ensure that messages are delivered one or more times, while "exactly once" delivery semantics guarantee that each message is delivered exactly once, even in the case of failures or retries. The latter requires additional configuration and overhead to achieve.

18. How does message acknowledgment work in ActiveMQ?
In ActiveMQ, message acknowledgment can be handled automatically or manually. In manual acknowledgment, the consumer explicitly sends an acknowledgment after processing the message. This ensures that messages are only removed from the queue once they have been successfully processed.

19. How do you ensure message ordering in Kafka?
Kafka ensures message ordering within a single partition. Messages sent to the same partition are delivered in the same order they were produced. However, Kafka does not guarantee order across different partitions of the same topic.

20. What is the role of a consumer in RabbitMQ, and how does it consume messages?
A consumer in RabbitMQ connects to the broker and retrieves messages from a queue. The consumer can either acknowledge the message immediately or later, depending on its processing logic. Consumers can be configured to consume messages in batches or one at a time.

Set 3: Message Brokers - RabbitMQ, Kafka, ActiveMQ
21. What are the different types of exchanges in RabbitMQ?
RabbitMQ supports different types of exchanges, including direct, topic, fanout, and headers exchanges. A direct exchange routes messages with an exact match of routing keys, while a topic exchange routes messages based on wildcard matching. A fanout exchange broadcasts messages to all queues bound to it, and a headers exchange routes messages based on header attributes.

22. How does Kafka's architecture handle fault tolerance?
Kafka handles fault tolerance through replication. Each partition of a Kafka topic is replicated across multiple brokers, ensuring that if one broker fails, the data is still available on another broker. Kafka also allows configurable replication factors to control the number of replicas.

23. How does ActiveMQ handle message priority?
ActiveMQ supports message priority, where messages are assigned a priority value (from 0 to 9). Higher priority messages are consumed first. This allows applications to prioritize certain types of messages over others for faster processing.

24. How does RabbitMQ handle message routing?
RabbitMQ uses exchanges to route messages to queues. The routing of messages is determined by the type of exchange (direct, topic, fanout, headers) and the routing key associated with each message. The exchange decides which queues should receive the message based on these rules.

25. What is Kafka's producer and consumer model?
Kafka follows a publish-subscribe model where the producer sends messages to topics and consumers read messages from those topics. A producer is responsible for writing messages, while the consumer reads the messages from one or more partitions. Kafka consumers can belong to a consumer group to share message consumption.

26. How does ActiveMQ ensure message delivery guarantees?
ActiveMQ ensures message delivery guarantees through different acknowledgement modes. The "auto-acknowledge" mode ensures that messages are automatically acknowledged when consumed, while "client-acknowledge" requires the consumer to explicitly acknowledge the message. Additionally, persistent messages are stored to ensure delivery even in case of broker failures.

27. How does Kafka handle scaling consumer applications?
Kafka scales consumer applications by distributing the load across multiple consumers in a consumer group. Each consumer in the group is assigned a subset of the partitions, ensuring parallel processing of messages. Kafka handles this by dynamically balancing the partitions between consumers.

28. What is a dead-letter queue (DLQ) in RabbitMQ, and when is it used?
A dead-letter queue (DLQ) in RabbitMQ is a queue where messages are sent when they cannot be delivered or processed successfully. This may occur due to message expiration, rejection by consumers, or exceeding the maximum queue length. DLQs are used for later inspection or reprocessing of undeliverable messages.

29. How do you monitor Kafka clusters?
Kafka clusters can be monitored using tools such as Kafka Manager, Burrow, and Prometheus. These tools provide metrics on broker health, consumer lag, topic throughput, and other key performance indicators. Kafka also exposes JMX metrics that can be integrated with monitoring systems for real-time monitoring.

30. What is a message queue's TTL (Time to Live), and how is it used in RabbitMQ?
TTL (Time to Live) in RabbitMQ refers to the time duration for which a message remains in a queue before it is discarded. TTL can be set on a per-message or per-queue basis. This feature is useful for ensuring that messages are not retained longer than necessary, preventing message buildup in queues.

31. What is the difference between RabbitMQ's "direct" and "topic" exchanges?
The "direct" exchange in RabbitMQ routes messages to queues based on an exact match between the routing key and the queue's binding key. In contrast, the "topic" exchange uses wildcard patterns in the routing key to match one or more queues, offering more flexibility in routing messages to multiple queues.

32. What is Kafka's partitioning model, and why is it important for scalability?
Kafka's partitioning model divides topics into multiple partitions, allowing messages to be distributed across different brokers. This model is crucial for scalability because it enables parallel processing by allowing consumers to read from different partitions simultaneously. It also ensures that Kafka can handle a large number of messages efficiently.

33. How does ActiveMQ handle transactions?
ActiveMQ supports transactions by allowing producers and consumers to group messages into a single unit of work. If a transaction is committed, all messages within the transaction are successfully processed. If the transaction is rolled back, none of the messages are processed, ensuring atomicity and consistency.

34. How does Kafka handle consumer group offsets?
In Kafka, each consumer group maintains its own offset, which tracks the position of the consumer within a partition. Kafka stores the offset in a special internal topic called "__consumer_offsets." When a consumer in a group reads a message, it updates its offset, ensuring that the next consumer reads the next message in the partition.

35. What are some strategies for securing communication in RabbitMQ?
RabbitMQ supports several strategies for securing communication, including SSL/TLS encryption for data in transit, authentication using username/password or client certificates, and authorization via Access Control Lists (ACLs) to manage permissions for different users and applications.

36. What is the role of the Zookeeper in Kafka?
Zookeeper in Kafka is used for distributed coordination. It manages broker metadata, partitions, and consumer offsets. Zookeeper ensures that Kafka brokers remain in sync and helps with leader election for partitions, making Kafka a distributed and fault-tolerant system.

37. How does ActiveMQ handle message persistence?
ActiveMQ handles message persistence by storing messages on disk in a durable format. This ensures that messages are not lost in case of broker failure. Persistent messages are saved to the disk before being acknowledged, ensuring reliability and durability.

38. How can you achieve message deduplication in Kafka?
Message deduplication in Kafka can be achieved by using idempotent producers and setting a unique message identifier. Kafka ensures that messages with the same producer id and message key are not written to the topic more than once, preventing duplicate messages from being stored or processed.

39. What is a "consumer lag" in Kafka?
Consumer lag in Kafka refers to the difference between the latest offset in a partition and the offset of the last message consumed by a consumer. A high consumer lag indicates that consumers are not processing messages as quickly as they are being produced, potentially leading to delays or backlogs.

40. How does RabbitMQ handle message expiration?
RabbitMQ handles message expiration by setting the `expiration` property on messages or queues. When a message expires, it is automatically removed from the queue. This is useful for ensuring that messages are processed within a certain timeframe or discarded if they are no longer relevant.

41. How does RabbitMQ handle message acknowledgments?
In RabbitMQ, messages can be acknowledged manually or automatically. If a consumer successfully processes a message, it sends an acknowledgment to RabbitMQ, signaling that the message has been handled. If a message is not acknowledged, RabbitMQ can requeue it or move it to a dead-letter queue, depending on the configuration.

42. How do you implement message retention in Kafka?
Message retention in Kafka is controlled by the topic's configuration, specifically the `retention.ms` setting. This setting defines the time period for which messages are retained in the topic. Once the retention period expires, the messages are deleted. Kafka also supports log compaction to retain the latest message for each key.

43. What are "queues" in RabbitMQ, and how do they function?
In RabbitMQ, queues are buffers that store messages until they are consumed by consumers. When a producer sends a message to RabbitMQ, the message is routed to one or more queues. Consumers then fetch messages from the queues for processing. Queues ensure that messages are stored reliably and can be processed asynchronously.

44. How does Kafka ensure message ordering?
Kafka guarantees message ordering within a partition. Each partition is an ordered, immutable sequence of messages, and Kafka ensures that consumers read messages in the same order they were produced. However, there is no ordering guarantee across partitions; to achieve global ordering, all messages must be written to the same partition.

45. What is the difference between ActiveMQ's "persistent" and "non-persistent" messages?
Persistent messages in ActiveMQ are stored on disk, ensuring that messages are not lost even in the event of broker failure. Non-persistent messages are not stored on disk and can be lost if the broker crashes. Persistent messages are slower to process due to the additional disk I/O but provide greater reliability.

46. How does Kafka handle topic creation and configuration?
Kafka automatically creates topics when producers send messages to a non-existent topic (if the `auto.create.topics.enable` property is set to true). Topics can also be manually created by administrators with specific configurations, such as the number of partitions and replication factor, using the Kafka command-line tools or APIs.

47. What is "queue binding" in RabbitMQ?
Queue binding in RabbitMQ refers to the association of a queue with an exchange using a binding key. This key is used by the exchange to determine which messages should be routed to the queue. The binding can be modified or removed at any time to change how messages are routed.

48. How does Kafka handle high throughput?
Kafka achieves high throughput by distributing data across multiple partitions and using a distributed architecture. Producers can write to multiple partitions in parallel, and consumers can read from different partitions simultaneously. Kafka also employs efficient storage formats and compression techniques to maximize throughput.

49. What is ActiveMQ's "virtual destination" feature?
ActiveMQ's virtual destination feature allows a single message to be consumed by multiple consumers, even if the consumers are consuming from different queues or topics. Virtual destinations enable more flexible message distribution, as a single message can be routed to different parts of the system based on configuration.

50. How does RabbitMQ support message routing based on headers?
RabbitMQ supports routing based on message headers using the "headers" exchange type. In this case, the exchange routes messages to queues based on matching header attributes rather than routing keys. This allows for more complex routing scenarios where the routing decision depends on the headers in the message.

51. How does Kafka handle message rebalancing in consumer groups?
In Kafka, message rebalancing occurs when a consumer group changes its composition, such as when a new consumer joins or an existing one leaves. Kafka automatically redistributes partitions among consumers to ensure an even load. This process may cause a brief interruption in message processing, but Kafka tries to minimize it.

52. What is RabbitMQ's "dead-letter exchange" (DLX), and how is it used?
A dead-letter exchange (DLX) in RabbitMQ is an exchange to which messages are sent when they are rejected, expired, or fail to be delivered. DLXs help manage failed messages and can be used to log, retry, or perform other actions on messages that couldn't be processed successfully.

53. What is the difference between Kafka's "replication factor" and "min.insync.replicas"?
The "replication factor" in Kafka determines how many copies of each partition are stored across different brokers, ensuring data redundancy. The "min.insync.replicas" setting specifies the minimum number of replicas that must be in sync to successfully write a message, ensuring data durability and availability.

54. How does ActiveMQ handle message delivery guarantees?
ActiveMQ provides message delivery guarantees through the use of "persistent" and "non-persistent" messages. Persistent messages are guaranteed to be delivered even if the broker crashes, while non-persistent messages might be lost in such cases. ActiveMQ also supports "acknowledgment" and "transactional" delivery modes for ensuring reliable message processing.

55. How can you handle high availability in RabbitMQ?
RabbitMQ achieves high availability through mirrored queues. These queues are replicated across multiple nodes in a cluster, so if one node fails, the messages in the mirrored queues are still accessible from another node. This ensures that RabbitMQ remains available even during node failures.

56. What is Kafka's "Log Compaction" feature, and how does it work?
Kafka's log compaction feature allows it to retain only the most recent message for each key, even after older messages have been deleted based on retention policies. This is useful for applications that need to keep the latest state of data rather than all events, such as when tracking user profiles or product inventories.

57. How does ActiveMQ handle message priority?
ActiveMQ allows producers to set message priorities using an integer value, with a higher number indicating a higher priority. When multiple consumers are available, messages with higher priority are delivered first. This feature ensures that important messages are processed before less urgent ones.

58. How does Kafka handle data loss in case of broker failure?
Kafka handles data loss through its replication model. When a message is produced, it is written to multiple replicas across different brokers. If a broker fails, Kafka can still provide access to the data from another replica, ensuring that no messages are lost, as long as a sufficient number of replicas are available.

59. What is RabbitMQ's "exchange-to-exchange" binding?
Exchange-to-exchange binding in RabbitMQ allows messages from one exchange to be routed to another exchange before reaching the final queue. This is useful for implementing complex routing patterns, such as chaining multiple exchanges for advanced message processing.

60. What is the role of "Kafka Connect" in the Kafka ecosystem?
Kafka Connect is a tool for integrating Kafka with external systems, such as databases, file systems, or other message brokers. It provides pre-built connectors for common systems and allows users to easily move data in and out of Kafka without writing custom code. Kafka Connect simplifies integration and scalability for Kafka deployments.

61. How does Kafka ensure that each message is processed only once?
Kafka ensures that each message is processed only once using the concept of "offsets." Each consumer in a consumer group keeps track of the last processed offset. This allows consumers to resume reading from the exact point they left off, ensuring that messages are not reprocessed unless explicitly configured to do so (e.g., when offsets are reset).

62. What is a "fanout" exchange in RabbitMQ?
A "fanout" exchange in RabbitMQ broadcasts messages to all queues that are bound to it, without considering the routing key. This is ideal when you want to send the same message to multiple consumers, regardless of any specific criteria. It's often used in scenarios where all subscribers need to receive every message.

63. How does ActiveMQ handle message groups?
ActiveMQ supports message groups to ensure that messages belonging to the same group are processed by the same consumer. This helps in maintaining message order for related messages. Consumers can be assigned a specific group, and ActiveMQ ensures that only one consumer handles all messages from that group, even when multiple consumers are available.

64. How do you manage consumer lag in Kafka?
Consumer lag in Kafka can be monitored by comparing the offset of the last processed message by a consumer with the offset of the latest message in the partition. If a consumer falls behind, you can scale up consumers, increase the number of partitions, or tune consumer configurations like `fetch.min.bytes` and `max.poll.records` to improve performance.

65. What is RabbitMQ's "topic exchange" used for?
A "topic exchange" in RabbitMQ is used for routing messages based on wildcard matching of routing keys. It allows for more granular control over message routing, enabling producers to send messages to queues based on specific patterns. For example, messages with the routing key "user.created" can be routed to a queue that handles user creation events.

66. How does Kafka handle scaling with partitions?
Kafka scales horizontally by partitioning topics. Each partition can be hosted on a different broker, and producers can write to different partitions in parallel. This enables Kafka to handle high throughput. Consumers can also read from multiple partitions simultaneously, enabling efficient parallel processing and scaling of the system.

67. How does ActiveMQ handle message selectors?
ActiveMQ supports message selectors, which allow consumers to filter messages based on message properties using SQL-like expressions. This helps consumers to selectively process messages that meet certain criteria, such as selecting only messages with a specific priority or type, without needing to process all incoming messages.

68. What is the significance of "producer acks" in Kafka?
Producer acknowledgments (acks) in Kafka determine how many replicas must acknowledge a message before the producer receives an acknowledgment. The "acks" setting can be configured as follows:

0: No acknowledgment required.
1: The leader replica must acknowledge.
all (or -1): All in-sync replicas must acknowledge.
This setting allows you to balance durability with performance, depending on your needs.

69. How can you ensure message durability in ActiveMQ?
Message durability in ActiveMQ can be ensured by setting messages as "persistent." Persistent messages are stored on disk, meaning they will not be lost in case of broker failure. Additionally, durable subscriptions can be created to retain messages even if the subscriber is not active when the message is sent.

70. How does Kafka manage the concept of "exactly-once" delivery?
Kafka provides "exactly-once" semantics (EOS) for both producers and consumers to ensure that messages are neither lost nor duplicated during processing. This is achieved through a combination of idempotent producers, transactions, and consumer offset management. With EOS enabled, Kafka guarantees that a message will be delivered and processed only once, even in the case of retries.

71. What is the role of the replication factor in Kafka?
The replication factor in Kafka determines how many copies of a partition are maintained across different brokers. A higher replication factor improves fault tolerance by ensuring that if a broker fails, another replica can take over as the leader, preventing data loss.

72. How does RabbitMQ handle priority queues?
RabbitMQ supports priority queues by allowing messages to have a priority level. Consumers process messages based on their priority, ensuring that higher-priority messages are delivered first. This is useful for scenarios where some messages need to be handled more urgently than others.

73. What is the difference between a Kafka log segment and a log partition?
A Kafka log partition is a division of a topic that allows parallel processing by multiple consumers. A log segment, on the other hand, is a subdivision of a partition that helps in log retention and deletion policies. Each partition consists of multiple log segments.

74. How does ActiveMQ support clustering?
ActiveMQ supports clustering using network connectors, which allow multiple brokers to form a network and share messages across nodes. This provides high availability, load balancing, and fault tolerance by allowing messages to be forwarded between brokers.

75. What is the role of consumer groups in Kafka?
Consumer groups in Kafka allow multiple consumers to read from a topic in parallel. Each partition is assigned to a single consumer within the group, ensuring that messages are evenly distributed among available consumers for parallel processing.

76. How do you ensure FIFO (First-In-First-Out) processing in RabbitMQ?
FIFO processing in RabbitMQ is ensured by using a single queue with a single consumer. Since RabbitMQ processes messages in the order they arrive, using a single consumer prevents out-of-order execution.

77. What are Kafka’s internal topics?
Kafka has several internal topics such as `__consumer_offsets`, which tracks consumer progress, and `__transaction_state`, which manages transaction states. These topics are used for internal Kafka operations like consumer group tracking and exactly-once semantics.

78. How does ActiveMQ handle dead-letter queues?
ActiveMQ automatically moves messages to a Dead Letter Queue (DLQ) when they fail to be processed after a specified number of retries. This allows for later inspection and debugging of failed messages.

79. What is the role of ZooKeeper in Kafka?
ZooKeeper is used in Kafka for leader election, maintaining metadata about brokers, topics, and partitions, and handling failover. Kafka is transitioning away from ZooKeeper with KRaft (Kafka Raft) to simplify deployment.

80. How does RabbitMQ handle message acknowledgments?
RabbitMQ allows consumers to acknowledge messages either automatically or manually. With manual acknowledgment, a consumer explicitly confirms message processing using `channel.basicAck()`, ensuring messages are not lost even if the consumer crashes.

81. What is the purpose of idempotence in Kafka producers?
Kafka producers use idempotence to ensure that duplicate messages are not written to a topic, even if retries occur due to network failures. This is achieved by assigning unique sequence numbers to messages.

82. How does RabbitMQ handle persistent messages?
RabbitMQ can store persistent messages by setting the message delivery mode to `2` and ensuring that the queue is durable. This prevents message loss in case of a broker restart.

83. What is the function of Kafka Streams?
Kafka Streams is a client library for real-time stream processing of data stored in Kafka topics. It allows developers to build stateful applications that process and transform data in a distributed manner.

84. How does ActiveMQ support transactions?
ActiveMQ supports transactions using JMS transactions and XA transactions. This allows message consumption and production to be grouped into atomic operations, ensuring reliability.

85. What is the purpose of a dead-letter exchange in RabbitMQ?
A dead-letter exchange (DLX) in RabbitMQ is used to handle messages that cannot be delivered to their intended queue. It allows failed messages to be redirected for later analysis or retries.

86. How does Kafka handle exactly-once semantics?
Kafka ensures exactly-once processing using transactional producers and idempotent consumers. This prevents duplicate messages or lost transactions in case of failures.

87. What is ActiveMQ Artemis?
ActiveMQ Artemis is a high-performance, next-generation messaging broker designed for better scalability and low latency. It is the recommended successor to traditional ActiveMQ.

88. How does RabbitMQ implement delayed message delivery?
RabbitMQ implements delayed message delivery using the "x-delayed-message" plugin, which allows messages to be published with a delay before reaching their destination queue.

89. How does Kafka support horizontal scaling?
Kafka supports horizontal scaling by partitioning topics and allowing multiple consumers to read in parallel. Adding more brokers distributes the workload efficiently.

90. What is the purpose of message selectors in ActiveMQ?
Message selectors in ActiveMQ allow consumers to filter messages based on specific properties using SQL-like expressions, reducing unnecessary message processing.

91. How does Kafka achieve durability of messages?
Kafka ensures message durability by writing messages to disk and replicating them across multiple brokers. The replication factor determines how many copies exist to prevent data loss.

92. What is RabbitMQ’s Lazy Queue?
Lazy Queues in RabbitMQ store messages on disk instead of RAM to handle high volumes of messages efficiently, reducing memory pressure and improving system stability.

93. How does Kafka handle log retention?
Kafka uses log retention policies to delete old messages based on time (`log.retention.hours`) or size (`log.retention.bytes`). Messages remain available until these limits are reached.

94. What is ActiveMQ Virtual Topics?
Virtual Topics in ActiveMQ allow multiple consumers to receive a copy of a message while still using a topic-based approach. This combines the benefits of queues and topics.

95. What is a quorum-based replication in Kafka?
Kafka’s quorum-based replication (introduced with KRaft) eliminates the need for ZooKeeper by using a Raft-based consensus algorithm for leader election and metadata management.

96. How do RabbitMQ shovels work?
Shovels in RabbitMQ are plugins that transfer messages from one broker to another, useful for federated RabbitMQ setups or data migration between clusters.

97. How does Kafka ensure ordering of messages?
Kafka guarantees message ordering within a partition. Consumers reading from the same partition always receive messages in the order they were produced.

98. What is an ActiveMQ network of brokers?
A network of brokers in ActiveMQ is a cluster of brokers connected together, allowing messages to be forwarded dynamically between nodes for scalability and fault tolerance.

99. What is the purpose of Kafka Connect?
Kafka Connect is a tool for streaming data between Kafka and external systems such as databases and cloud storage, making integration easier with prebuilt connectors.

100. How does RabbitMQ handle multiple consumers on a queue?
RabbitMQ uses a round-robin distribution mechanism where messages are distributed equally among multiple consumers attached to the same queue.