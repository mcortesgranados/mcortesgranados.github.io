<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Spark Shell and Spark-submit</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
    </style>
</head>
<body>
    <h1>Introduction to Spark Shell and Spark-submit</h1>
    <p>This guide provides an introduction to Spark Shell and <code>spark-submit</code>, including how to use them to interact with Apache Spark and submit applications.</p>

    <h2>What is Spark Shell?</h2>
    <p>Spark Shell is an interactive REPL (Read-Eval-Print Loop) that allows you to run Spark commands and perform data analysis in an interactive environment. It supports both Scala and Python (PySpark) and is useful for testing, prototyping, and data exploration.</p>

    <h3>Using Spark Shell</h3>
    <ul>
        <li><strong>Starting Spark Shell:</strong> You can start Spark Shell from the command line. For Scala, use <code>spark-shell</code>, and for Python, use <code>pyspark</code>.</li>
        <li><strong>Example (Scala):</strong> <code>spark-shell</code></li>
        <li><strong>Example (Python):</strong> <code>pyspark</code></li>
    </ul>
    <p>Once started, you can run Spark commands directly. For example, you can create an RDD, perform transformations, and actions.</p>

    <h2>What is spark-submit?</h2>
    <p><code>spark-submit</code> is a command-line tool used to submit Spark applications to a cluster. It allows you to run a Spark job on a cluster or in local mode, specifying various configurations and dependencies.</p>

    <h3>Using spark-submit</h3>
    <ul>
        <li><strong>Basic Syntax:</strong> <code>spark-submit [options] <app-jar> [app-arguments]</code></li>
        <li><strong>Example:</strong> To submit a Spark application written in Scala:</li>
    </ul>
    <pre><code>
spark-submit --class com.example.MySparkApp \
    --master local[4] \
    --deploy-mode client \
    my-spark-app.jar \
    arg1 arg2
    </code></pre>

    <h2>Example: Running Spark Shell and spark-submit</h2>
    <h3>Running Spark Shell</h3>
    <pre><code>
# Start Spark Shell in Scala
spark-shell

# Start Spark Shell in Python
pyspark
    </code></pre>
    <p>In Spark Shell, you can execute Spark commands interactively. For example:</p>
    <pre><code>
# Create an RDD
val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))

# Perform a transformation
val squaredRDD = rdd.map(x => x * x)

# Perform an action
squaredRDD.collect().foreach(println)
    </code></pre>

    <h3>Using spark-submit</h3>
    <p>Suppose you have a Spark application packaged in a JAR file. You can submit it using <code>spark-submit</code>:</p>
    <pre><code>
spark-submit --class com.example.MySparkApp \
    --master local[4] \
    --deploy-mode client \
    my-spark-app.jar \
    arg1 arg2
    </code></pre>
    <p>In this example:</p>
    <ul>
        <li><strong>--class:</strong> Specifies the main class of the application.</li>
        <li><strong>--master:</strong> Defines the Spark master URL (e.g., local mode or cluster mode).</li>
        <li><strong>--deploy-mode:</strong> Indicates whether the application is deployed in client or cluster mode.</li>
        <li><strong>my-spark-app.jar:</strong> The JAR file containing the Spark application.</li>
        <li><strong>arg1 arg2:</strong> Arguments passed to the application.</li>
    </ul>

    <h2>Conclusion</h2>
    <p>Spark Shell is a powerful tool for interactive data analysis and exploration, while <code>spark-submit</code> is essential for running Spark applications in a distributed environment. Both tools are integral to working effectively with Apache Spark.</p>
</body>
</html>
