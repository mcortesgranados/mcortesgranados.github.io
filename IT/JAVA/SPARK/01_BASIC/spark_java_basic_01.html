<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Big Data with Apache Spark</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Introduction to Big Data with Apache Spark</h1>

    <p><strong>Apache Spark</strong> is an open-source, distributed computing system that provides a unified processing engine for large-scale data processing. It offers a fast and general-purpose cluster-computing framework for big data analytics. Spark can handle both batch and stream processing and supports various programming languages, including Java, Scala, Python, and R.</p>

    <h2>Key Features of Apache Spark:</h2>
    <ul>
        <li><strong>Speed:</strong> In-memory computation allows Spark to process data much faster than traditional disk-based engines.</li>
        <li><strong>Ease of Use:</strong> Provides high-level APIs in Java, Scala, Python, and R, and includes built-in libraries for SQL, streaming, machine learning, and graph processing.</li>
        <li><strong>Flexibility:</strong> Can be deployed on various cluster managers, including Hadoop YARN, Apache Mesos, and Kubernetes.</li>
    </ul>

    <h2>Example Code in Java:</h2>
    <p>The following code snippet demonstrates a simple Spark application in Java that performs basic data operations:</p>

    <pre><code class="language-java">
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;

public class SparkExample {
    public static void main(String[] args) {
        // Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Spark Example").setMaster("local");
        
        // Create a JavaSparkContext object with the SparkConf
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Create an RDD (Resilient Distributed Dataset) from a list of integers
        JavaRDD<Integer> numbers = sc.parallelize(java.util.Arrays.asList(1, 2, 3, 4, 5));

        // Perform a transformation (map) and an action (reduce)
        int sum = numbers.map(number -> number * 2) // Double each number
                          .reduce((a, b) -> a + b); // Sum the numbers

        // Print the result
        System.out.println("Sum of doubled numbers: " + sum);

        // Stop the SparkContext
        sc.close();
    }
}
    </code></pre>

    <p>This example creates a Spark application that initializes a Spark context, processes a dataset by doubling the numbers, and then calculates the sum of the doubled numbers.</p>
</body>
</html>
