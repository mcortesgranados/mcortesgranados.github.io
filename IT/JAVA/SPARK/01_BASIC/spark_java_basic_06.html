<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Installing Spark on Local Machine</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Installing Spark on Local Machine</h1>

    <p>Installing Apache Spark on a local machine is a straightforward process. This guide will walk you through downloading, installing, and configuring Spark to get it up and running on your system.</p>

    <h2>1. Prerequisites</h2>
    <ul>
        <li><strong>Java:</strong> Apache Spark requires Java 8 or later. Ensure that Java is installed and JAVA_HOME is set up. You can check your Java installation with:
            <pre><code>$ java -version</code></pre>
        </li>
        <li><strong>Scala:</strong> While not strictly necessary, having Scala installed can be beneficial as Spark is written in Scala.</li>
    </ul>

    <h2>2. Download Apache Spark</h2>
    <ol>
        <li>Go to the <a href="https://spark.apache.org/downloads.html" target="_blank">Apache Spark download page</a>.</li>
        <li>Select a Spark release (e.g., 3.4.0) and choose a package type (e.g., pre-built for Hadoop 3.2 and later).</li>
        <li>Download the selected Spark binary archive (e.g., `spark-3.4.0-bin-hadoop3.2.tgz`).</li>
    </ol>

    <h2>3. Install Apache Spark</h2>
    <ol>
        <li>Extract the downloaded archive to a directory of your choice:
            <pre><code>$ tar xzf spark-3.4.0-bin-hadoop3.2.tgz</code></pre>
        </li>
        <li>Move the extracted directory to a preferred location (e.g., `/usr/local/spark`):
            <pre><code>$ sudo mv spark-3.4.0-bin-hadoop3.2 /usr/local/spark</code></pre>
        </li>
        <li>Add Spark's `bin` directory to your system's PATH environment variable. You can add the following line to your `~/.bashrc` or `~/.zshrc` file:
            <pre><code>export PATH=$PATH:/usr/local/spark/bin</code></pre>
        </li>
    </ol>

    <h2>4. Configure Apache Spark</h2>
    <ol>
        <li>Navigate to the `conf` directory within the Spark installation directory:
            <pre><code>$ cd /usr/local/spark/conf</code></pre>
        </li>
        <li>Copy the template configuration file:
            <pre><code>$ cp spark-env.sh.template spark-env.sh</code></pre>
        </li>
        <li>Edit `spark-env.sh` to set necessary environment variables, such as:
            <pre><code>export SPARK_MASTER_HOST='localhost'</code></pre>
        </li>
    </ol>

    <h2>5. Verify the Installation</h2>
    <p>To verify that Spark is installed correctly, run the Spark shell:
        <pre><code>$ spark-shell</code></pre>
    </p>
    <p>If the Spark shell starts successfully, the installation is complete. You can exit the shell by typing <code>exit</code>.</p>

    <h2>6. Example Code in Java</h2>
    <p>The following Java code demonstrates a basic Spark application that initializes a Spark context, creates an RDD, and performs a simple operation:</p>

    <pre><code class="language-java">
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class SparkInstallationExample {
    public static void main(String[] args) {
        // Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Spark Installation Example").setMaster("local");
        
        // Create a JavaSparkContext with the configuration
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Create an RDD from a list of integers
        JavaRDD<Integer> numbers = sc.parallelize(java.util.Arrays.asList(1, 2, 3, 4, 5));

        // Perform a transformation and action
        JavaRDD<Integer> doubledNumbers = numbers.map(number -> number * 2);
        doubledNumbers.collect().forEach(System.out::println);

        // Stop the SparkContext
        sc.close();
    }
}
    </code></pre>

    <p>This example sets up a Spark application, creates an RDD, performs a map operation to double the numbers, and prints the results. It is a good starting point to test your Spark setup.</p>
</body>
</html>
