<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Apache Spark</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Introduction to Apache Spark</h1>

    <p><strong>Apache Spark</strong> is an open-source unified analytics engine designed for large-scale data processing. It provides high-speed processing through in-memory computation, supports a variety of data processing tasks, and is highly versatile in its deployment and usage. Spark can be used for batch processing, real-time stream processing, and iterative machine learning tasks.</p>

    <h2>Key Features of Apache Spark:</h2>
    <ul>
        <li><strong>High Performance:</strong> Spark processes data in-memory, which significantly speeds up processing times compared to traditional disk-based systems.</li>
        <li><strong>Unified Analytics Engine:</strong> Spark integrates various data processing capabilities, including SQL querying, streaming data, machine learning, and graph processing.</li>
        <li><strong>Ease of Integration:</strong> Spark can work with various data sources such as HDFS, Apache Cassandra, Apache HBase, and Amazon S3.</li>
        <li><strong>Rich APIs:</strong> Provides APIs in Java, Scala, Python, and R, making it accessible to a wide range of developers.</li>
    </ul>

    <h2>Example Code in Java:</h2>
    <p>The following Java code demonstrates a basic Spark application that creates an RDD (Resilient Distributed Dataset), performs a transformation, and executes an action:</p>

    <pre><code class="language-java">
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class SparkExample {
    public static void main(String[] args) {
        // Set up the Spark configuration
        SparkConf conf = new SparkConf().setAppName("Apache Spark Example").setMaster("local");
        
        // Create a JavaSparkContext with the configuration
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Create an RDD from a list of integers
        JavaRDD<Integer> numbers = sc.parallelize(java.util.Arrays.asList(1, 2, 3, 4, 5));

        // Transform the RDD by doubling each number
        JavaRDD<Integer> doubledNumbers = numbers.map(number -> number * 2);

        // Collect the results and print them
        System.out.println("Doubled numbers: " + doubledNumbers.collect());

        // Stop the SparkContext
        sc.close();
    }
}
    </code></pre>

    <p>This example initializes a Spark context, creates an RDD, applies a transformation to double the numbers, and collects and prints the results.</p>
</body>
</html>
