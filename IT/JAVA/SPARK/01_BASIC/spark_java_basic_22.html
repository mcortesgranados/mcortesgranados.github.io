<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reading and Writing Files in Spark with Java</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
    </style>
</head>
<body>
    <h1>Reading and Writing Files in Spark with Java</h1>
    <p>This guide demonstrates how to read from and write to files in Apache Spark using Java.</p>

    <h2>Reading Files in Spark</h2>
    <p>In Spark, you can read data from various file formats such as text, CSV, JSON, and Parquet. The <code>SparkSession</code> provides methods to read these file types and create DataFrames or RDDs.</p>

    <h2>Writing Files in Spark</h2>
    <p>Similarly, Spark allows you to write DataFrames or RDDs to different file formats. You can specify the format and path where the data should be saved.</p>

    <h2>Java Code Example</h2>
    <pre><code>
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class FileIOExample {
    public static void main(String[] args) {
        // Create SparkSession
        SparkSession spark = SparkSession.builder()
            .appName("Reading and Writing Files Example")
            .master("local[*]")
            .getOrCreate();

        // Reading a text file into an RDD
        JavaRDD&lt;String&gt; textFileRDD = spark.read().textFile("data/input.txt").javaRDD();
        System.out.println("Number of lines in text file: " + textFileRDD.count());

        // Reading a JSON file into a DataFrame
        Dataset&lt;Row&gt; jsonDF = spark.read().json("data/input.json");
        jsonDF.show();

        // Writing a DataFrame to a Parquet file
        jsonDF.write().parquet("data/output.parquet");

        // Writing an RDD to a text file
        textFileRDD.saveAsTextFile("data/output.txt");

        // Stop the SparkSession
        spark.stop();
    }
}
    </code></pre>

    <h2>Understanding the Example</h2>
    <p>In this example, we demonstrate the following operations:</p>
    <ul>
        <li><strong>Reading a Text File:</strong> Use the <code>textFile</code> method to read a text file into an RDD. The <code>javaRDD</code> method converts the DataFrame to an RDD.</li>
        <li><strong>Reading a JSON File:</strong> Use the <code>json</code> method to read a JSON file into a DataFrame.</li>
        <li><strong>Writing to Parquet:</strong> Use the <code>write</code> method to save a DataFrame as a Parquet file.</li>
        <li><strong>Writing to Text File:</strong> Use the <code>saveAsTextFile</code> method to save an RDD as a text file.</li>
        <li><strong>Stopping SparkSession:</strong> Stop the <code>SparkSession</code> when done to free up resources.</li>
    </ul>

    <h2>Conclusion</h2>
    <p>Apache Spark provides powerful methods for reading from and writing to various file formats. By leveraging these capabilities, you can efficiently handle large datasets in your Spark applications. Understanding how to work with different file formats is crucial for building robust and scalable data processing pipelines.</p>
</body>
</html>
