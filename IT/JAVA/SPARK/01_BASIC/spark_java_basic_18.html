<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Loading Data from CSV, JSON, and Parquet Files in Spark</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
    </style>
</head>
<body>
    <h1>Loading Data from CSV, JSON, and Parquet Files in Spark</h1>
    <p>This guide explains how to load data from CSV, JSON, and Parquet files into Spark DataFrames using Java.</p>

    <h2>Loading Data from CSV Files</h2>
    <p>CSV (Comma-Separated Values) files are one of the most common formats for storing tabular data. Spark can read CSV files using its built-in functionality.</p>

    <h3>Example: Loading a CSV File</h3>
    <pre><code>
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class LoadCSVExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("CSV Loader")
                .master("local[*]")
                .getOrCreate();

        // Step 2: Load the CSV file into a DataFrame
        Dataset&lt;Row&gt; df = spark.read()
                .option("header", "true")  // Use the first row as header
                .option("inferSchema", "true")  // Automatically infer data types
                .csv("path/to/your/file.csv");

        // Step 3: Show the loaded data
        df.show();

        // Step 4: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>Loading Data from JSON Files</h2>
    <p>JSON (JavaScript Object Notation) is widely used for representing structured data. Spark can easily read JSON files and convert them into DataFrames.</p>

    <h3>Example: Loading a JSON File</h3>
    <pre><code>
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class LoadJSONExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("JSON Loader")
                .master("local[*]")
                .getOrCreate();

        // Step 2: Load the JSON file into a DataFrame
        Dataset&lt;Row&gt; df = spark.read()
                .json("path/to/your/file.json");

        // Step 3: Show the loaded data
        df.show();

        // Step 4: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>Loading Data from Parquet Files</h2>
    <p>Parquet is a columnar storage format that is highly efficient for reading and writing large datasets. It is optimized for performance, making it a preferred choice for large-scale data analytics.</p>

    <h3>Example: Loading a Parquet File</h3>
    <pre><code>
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class LoadParquetExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Parquet Loader")
                .master("local[*]")
                .getOrCreate();

        // Step 2: Load the Parquet file into a DataFrame
        Dataset&lt;Row&gt; df = spark.read()
                .parquet("path/to/your/file.parquet");

        // Step 3: Show the loaded data
        df.show();

        // Step 4: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>Understanding the Examples</h2>
    <p>In each of the examples, the following steps are followed:</p>
    <ol>
        <li><strong>Create a SparkSession:</strong> This is required to work with Spark DataFrames.</li>
        <li><strong>Read the file:</strong> Spark provides built-in methods for reading different file formats like <code>csv</code>, <code>json</code>, and <code>parquet</code>. You can also configure options like headers, delimiter, and schema inference.</li>
        <li><strong>Show the DataFrame:</strong> The <code>show()</code> method displays the contents of the DataFrame.</li>
    </ol>

    <h2>Additional Options for Loading Data</h2>
    <ul>
        <li><strong>header:</strong> When loading CSV files, use <code>option("header", "true")</code> to use the first row as headers.</li>
        <li><strong>inferSchema:</strong> For CSV files, <code>option("inferSchema", "true")</code> automatically infers the data types of the columns.</li>
        <li><strong>multiline:</strong> When loading JSON files, <code>option("multiline", "true")</code> enables reading multi-line JSON objects.</li>
        <li><strong>delimiter:</strong> For CSV files with a delimiter other than a comma, use <code>option("delimiter", "your_delimiter")</code>.</li>
    </ul>

    <h2>Conclusion</h2>
    <p>Spark makes it easy to load data from various file formats such as CSV, JSON, and Parquet into DataFrames. The ability to handle multiple file formats makes Spark a versatile tool for data processing and analysis. With options like schema inference and multiline reading, Spark offers flexibility in dealing with different types of structured data.</p>
</body>
</html>
