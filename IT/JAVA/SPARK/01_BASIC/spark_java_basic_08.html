<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to RDD (Resilient Distributed Datasets)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Introduction to RDD (Resilient Distributed Datasets)</h1>

    <p>Resilient Distributed Datasets (RDDs) are a fundamental data structure in Apache Spark. They represent a distributed collection of objects that can be processed in parallel across a cluster. RDDs are designed to be fault-tolerant, efficient, and easy to use. Here's an overview of RDDs and how they work:</p>

    <h2>What is an RDD?</h2>
    <p>An RDD is a read-only, partitioned collection of records that can be operated on in parallel. RDDs are immutable, meaning once created, they cannot be changed. Operations on RDDs create new RDDs rather than modifying existing ones.</p>

    <h2>Key Features of RDDs</h2>
    <ul>
        <li><strong>Resilience:</strong> RDDs are fault-tolerant. If a partition of an RDD is lost, Spark can recompute it using the lineage information.</li>
        <li><strong>Distributed:</strong> RDDs are distributed across the nodes of a cluster, allowing for parallel processing of data.</li>
        <li><strong>Immutable:</strong> Once created, RDDs cannot be modified. Transformations on RDDs result in new RDDs.</li>
        <li><strong>Lazy Evaluation:</strong> RDDs use lazy evaluation, meaning computations are deferred until an action is called. This optimizes the execution plan.</li>
    </ul>

    <h2>Common RDD Operations</h2>
    <ul>
        <li><strong>Transformations:</strong> Operations that create a new RDD from an existing one, such as <code>map()</code>, <code>filter()</code>, and <code>flatMap()</code>.</li>
        <li><strong>Actions:</strong> Operations that trigger computation and return results, such as <code>collect()</code>, <code>count()</code>, and <code>saveAsTextFile()</code>.</li>
    </ul>

    <h2>Example Code in Java</h2>
    <p>The following Java code demonstrates basic operations with RDDs. It creates an RDD from a list of integers, performs a map transformation, and collects the results:</p>

    <pre><code class="language-java">
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDExample {
    public static void main(String[] args) {
        // Create a SparkConf object and set the application name and master
        SparkConf conf = new SparkConf().setAppName("RDD Example").setMaster("local");
        
        // Create a JavaSparkContext with the configuration
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Create an RDD from a list of integers
        JavaRDD<Integer> numbers = sc.parallelize(java.util.Arrays.asList(1, 2, 3, 4, 5));

        // Perform a transformation: multiply each number by 2
        JavaRDD<Integer> doubledNumbers = numbers.map(number -> number * 2);

        // Collect and print the results
        doubledNumbers.collect().forEach(System.out::println);

        // Stop the SparkContext
        sc.close();
    }
}
    </code></pre>

    <p>This example sets up a Spark application, creates an RDD from a list of integers, performs a <code>map()</code> transformation to double each number, and prints the results. The <code>collect()</code> action triggers the computation and retrieves the results from the cluster.</p>

</body>
</html>
