<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Running Spark in Local Mode</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <h1>Running Spark in Local Mode</h1>

    <p>Running Apache Spark in local mode is ideal for development and testing purposes. It allows you to run Spark on your local machine without needing a distributed cluster. This guide will walk you through the steps to run Spark in local mode and provide a simple Java code example.</p>

    <h2>1. Prerequisites</h2>
    <ul>
        <li><strong>Apache Spark:</strong> Ensure that Spark is installed on your local machine. Refer to the installation guide if needed.</li>
        <li><strong>Java:</strong> Make sure Java is installed and properly set up on your machine.</li>
    </ul>

    <h2>2. Running Spark Shell in Local Mode</h2>
    <p>The Spark shell is an interactive environment for running Spark commands. To start Spark shell in local mode, use the following command:</p>
    <pre><code>$ spark-shell --master local</code></pre>
    <p>This command starts the Spark shell with a local master, meaning that Spark will run on a single machine.</p>

    <h2>3. Running Spark Applications in Local Mode</h2>
    <p>You can also run Spark applications in local mode by specifying the `local` master in your application's configuration. This is done using the `SparkConf` object in your code.</p>

    <h2>4. Example Code in Java</h2>
    <p>The following Java code demonstrates how to run a basic Spark application in local mode:</p>

    <pre><code class="language-java">
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class SparkLocalModeExample {
    public static void main(String[] args) {
        // Create a SparkConf object and set the application name and master
        SparkConf conf = new SparkConf().setAppName("Spark Local Mode Example").setMaster("local");
        
        // Create a JavaSparkContext with the configuration
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Create an RDD from a list of integers
        JavaRDD<Integer> numbers = sc.parallelize(java.util.Arrays.asList(1, 2, 3, 4, 5));

        // Perform a transformation and action
        JavaRDD<Integer> doubledNumbers = numbers.map(number -> number * 2);
        doubledNumbers.collect().forEach(System.out::println);

        // Stop the SparkContext
        sc.close();
    }
}
    </code></pre>

    <p>This example sets up a Spark application to run in local mode. It creates an RDD from a list of integers, performs a map transformation to double the numbers, and prints the results. To run this application:</p>
    <ol>
        <li>Compile the Java code into a JAR file.</li>
        <li>Submit the JAR file to Spark using the following command:
            <pre><code>$ spark-submit --class SparkLocalModeExample --master local path/to/your-application.jar</code></pre>
        </li>
    </ol>

    <p>Running this command will execute the Spark application in local mode and display the results in your terminal.</p>
</body>
</html>
