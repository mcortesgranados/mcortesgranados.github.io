<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Working with JSON and Parquet File Formats</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
        }
    </style>
</head>
<body>
    <h1>Working with JSON and Parquet File Formats</h1>
    <p>JSON and Parquet are popular file formats for data storage and processing. JSON is often used for data interchange and configuration, while Parquet is optimized for query performance and storage efficiency. This guide covers how to work with both formats using Apache Spark.</p>

    <h2>1. Working with JSON Files</h2>
    <p>JSON (JavaScript Object Notation) is a lightweight format commonly used for data interchange. Spark provides support for reading and writing JSON files, making it easy to process structured data.</p>

    <h3>Example: Reading and Writing JSON Files</h3>
    <pre><code>
import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class JsonExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("JSON Example").setMaster("local[*]");
        
        // Step 2: Create a SparkSession object
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        
        // Step 3: Read data from a JSON file
        Dataset&lt;Row&gt; df = spark.read().json("path/to/json/file");
        
        // Step 4: Show the DataFrame
        df.show();
        
        // Step 5: Write the DataFrame to a JSON file
        df.write().json("path/to/output/json");
        
        // Step 6: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>2. Working with Parquet Files</h2>
    <p>Parquet is a columnar storage file format designed to optimize performance and compression. It is well-suited for big data processing and can significantly reduce the size of data stored on disk.</p>

    <h3>Example: Reading and Writing Parquet Files</h3>
    <pre><code>
import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class ParquetExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Parquet Example").setMaster("local[*]");
        
        // Step 2: Create a SparkSession object
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        
        // Step 3: Read data from a Parquet file
        Dataset&lt;Row&gt; df = spark.read().parquet("path/to/parquet/file");
        
        // Step 4: Show the DataFrame
        df.show();
        
        // Step 5: Write the DataFrame to a Parquet file
        df.write().parquet("path/to/output/parquet");
        
        // Step 6: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>3. Comparison Table</h2>
    <table>
        <thead>
            <tr>
                <th>Aspect</th>
                <th>JSON</th>
                <th>Parquet</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Format Type</td>
                <td>Text-based</td>
                <td>Columnar</td>
            </tr>
            <tr>
                <td>Compression</td>
                <td>Less efficient</td>
                <td>Highly efficient</td>
            </tr>
            <tr>
                <td>Query Performance</td>
                <td>Slower for large datasets</td>
                <td>Faster due to columnar storage</td>
            </tr>
            <tr>
                <td>Use Cases</td>
                <td>Data interchange, configuration</td>
                <td>Big data processing, analytics</td>
            </tr>
        </tbody>
    </table>

    <h2>4. Conclusion</h2>
    <p>JSON and Parquet are useful file formats for different scenarios. JSON is ideal for data interchange and configuration, while Parquet offers efficient storage and query performance for large-scale data processing. Understanding how to work with both formats in Spark will help you choose the right format for your data processing needs.</p>
</body>
</html>
