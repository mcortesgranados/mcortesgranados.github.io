<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Working with Apache Kafka and Spark Streaming</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
        }
    </style>
</head>
<body>
    <h1>Working with Apache Kafka and Spark Streaming</h1>
    <p>Apache Kafka is a high-throughput, low-latency distributed messaging system. When combined with Spark Streaming, it allows you to process and analyze real-time data streams efficiently. This guide covers how to set up and use Kafka with Spark Streaming in Java.</p>

    <h2>1. Introduction to Kafka and Spark Streaming Integration</h2>
    <p>Integrating Kafka with Spark Streaming enables real-time processing of data streams. Kafka serves as the data source, and Spark Streaming processes these streams in near real-time, providing powerful analytics and transformation capabilities.</p>

    <h2>2. Example: Setting Up Kafka and Spark Streaming</h2>
    <p>The following example demonstrates how to set up a Spark Streaming application that consumes data from a Kafka topic and performs basic operations.</p>

    <h3>Example: Streaming from Kafka with Spark</h3>
    <pre><code>
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaOffsetRange;
import scala.Tuple2;

import java.util.HashMap;
import java.util.Map;

public class KafkaSparkStreamingExample {
    public static void main(String[] args) throws InterruptedException {
        // Step 1: Create a SparkConf and JavaStreamingContext
        SparkConf conf = new SparkConf().setAppName("Kafka Spark Streaming Example").setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        JavaStreamingContext ssc = new JavaStreamingContext(sc, new Duration(1000));
        
        // Step 2: Define Kafka parameters
        Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();
        kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, "spark-streaming-group");
        kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // Step 3: Create a DStream from Kafka
        JavaDStream&lt;String&gt; stream = KafkaUtils.createDirectStream(
            ssc,
            LocationStrategies.PreferConsistent(),
            ConsumerStrategies.Subscribe(Collections.singleton("my_topic"), kafkaParams)
        ).map(record -> record.value());
        
        // Step 4: Process the stream
        stream.foreachRDD(rdd -> {
            rdd.foreach(record -> System.out.println("Received record: " + record));
        });
        
        // Step 5: Start the streaming context and await termination
        ssc.start();
        ssc.awaitTermination();
    }
}
    </code></pre>

    <h2>3. Explanation of the Example</h2>
    <table>
        <thead>
            <tr>
                <th>Step</th>
                <th>Description</th>
                <th>Code</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1</td>
                <td>Create a SparkConf and JavaStreamingContext</td>
                <td><code>SparkConf conf = new SparkConf() ... ;<br>JavaStreamingContext ssc = new JavaStreamingContext(sc, new Duration(1000));</code></td>
            </tr>
            <tr>
                <td>2</td>
                <td>Define Kafka parameters</td>
                <td><code>Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;&gt;();<br>kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");</code></td>
            </tr>
            <tr>
                <td>3</td>
                <td>Create a DStream from Kafka</td>
                <td><code>JavaDStream&lt;String&gt; stream = KafkaUtils.createDirectStream(...).map(record -> record.value());</code></td>
            </tr>
            <tr>
                <td>4</td>
                <td>Process the stream</td>
                <td><code>stream.foreachRDD(rdd -> rdd.foreach(record -> System.out.println("Received record: " + record)));</code></td>
            </tr>
            <tr>
                <td>5</td>
                <td>Start the streaming context and await termination</td>
                <td><code>ssc.start();<br>ssc.awaitTermination();</code></td>
            </tr>
        </tbody>
    </table>

    <h2>4. Conclusion</h2>
    <p>Integrating Apache Kafka with Spark Streaming allows you to build powerful real-time data processing applications. By setting up Kafka as a data source and using Spark Streaming to process these data streams, you can efficiently analyze and act on real-time data, leveraging the strengths of both technologies.</p>
</body>
</html>
