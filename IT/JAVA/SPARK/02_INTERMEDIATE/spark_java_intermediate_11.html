<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Working with Spark Datasets</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
    </style>
</head>
<body>
    <h1>Working with Spark Datasets</h1>
    <p>Spark Datasets provide a powerful way to work with distributed data by combining the benefits of RDDs and DataFrames. They offer strong typing and compile-time type safety, along with the ability to perform SQL-like operations. This guide explains how to work with Spark Datasets, including examples in Java.</p>

    <h2>1. Creating Datasets</h2>
    <p>Datasets can be created from existing RDDs, DataFrames, or by reading data from external sources like files. The key aspect is to define a schema that represents the data structure.</p>

    <h3>Example: Creating Datasets from a Collection</h3>
    <pre><code>
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import org.apache.spark.sql.api.java.JavaRow;
import java.util.Arrays;
import java.util.List;

public class DatasetCreationExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Dataset Creation Example").setMaster("local[*]");
        
        // Step 2: Create a SparkSession object
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        
        // Step 3: Define a schema for the dataset
        StructType schema = DataTypes.createStructType(new StructField[]{
            DataTypes.createStructField("id", DataTypes.IntegerType, false),
            DataTypes.createStructField("name", DataTypes.StringType, false)
        });
        
        // Step 4: Create a list of data
        List&lt;Row&gt; data = Arrays.asList(
            RowFactory.create(1, "John"),
            RowFactory.create(2, "Jane")
        );
        
        // Step 5: Create a Dataset using the data and schema
        Dataset&lt;Row&gt; dataset = spark.createDataFrame(data, schema);
        
        // Step 6: Show the dataset
        dataset.show();
        
        // Step 7: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>2. Performing Transformations</h2>
    <p>Datasets support various transformations, including filtering, mapping, and aggregations. These transformations are similar to those available in RDDs and DataFrames but with type safety.</p>

    <h3>Example: Performing Transformations on Datasets</h3>
    <pre><code>
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class DatasetTransformationsExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Dataset Transformations Example").setMaster("local[*]");
        
        // Step 2: Create a SparkSession object
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        
        // Step 3: Read data from a JSON file into a Dataset
        Dataset&lt;Row&gt; dataset = spark.read().json("path/to/json/file");
        
        // Step 4: Perform transformations
        Dataset&lt;Row&gt; filteredDataset = dataset.filter(dataset.col("age").gt(21));
        Dataset&lt;Row&gt; selectedDataset = filteredDataset.select("name", "age");
        
        // Step 5: Show the results
        selectedDataset.show();
        
        // Step 6: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>3. Performing Actions</h2>
    <p>Actions in Datasets include operations that trigger computation and return results to the driver program, such as collecting data or writing it to an external storage system.</p>

    <h3>Example: Performing Actions on Datasets</h3>
    <pre><code>
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class DatasetActionsExample {
    public static void main(String[] args) {
        // Step 1: Create a SparkConf object and set the application name
        SparkConf conf = new SparkConf().setAppName("Dataset Actions Example").setMaster("local[*]");
        
        // Step 2: Create a SparkSession object
        SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
        
        // Step 3: Read data from a JSON file into a Dataset
        Dataset&lt;Row&gt; dataset = spark.read().json("path/to/json/file");
        
        // Step 4: Perform an action to collect data
        List&lt;Row&gt; rows = dataset.collectAsList();
        for (Row row : rows) {
            System.out.println(row);
        }
        
        // Step 5: Stop the Spark session
        spark.stop();
    }
}
    </code></pre>

    <h2>Understanding the Examples</h2>
    <p>In these examples, we demonstrate the following aspects of working with Spark Datasets:</p>
    <ul>
        <li><strong>Creating Datasets:</strong> Defining a schema and creating a Dataset from a collection or external data source.</li>
        <li><strong>Performing Transformations:</strong> Applying transformations like filtering and selecting columns.</li>
        <li><strong>Performing Actions:</strong> Collecting data or writing it to external storage.</li>
    </ul>

    <h2>Conclusion</h2>
    <p>Spark Datasets offer a powerful abstraction that combines the best features of RDDs and DataFrames. They provide strong typing, compile-time safety, and optimized performance, making them a valuable tool for distributed data processing.</p>
</body>
</html>
