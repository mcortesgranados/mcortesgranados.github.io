<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kafka Advanced: Microservices</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            color: #333;
        }
        h1, h2, h3 {
            color: #007acc;
        }
        code {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 5px;
        }
        pre {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            overflow-x: auto;
        }
        .note {
            background: #e7f4e4;
            border-left: 5px solid #4caf50;
            padding: 10px;
            margin: 10px 0;
        }
        .warning {
            background: #fff3e0;
            border-left: 5px solid #ff9800;
            padding: 10px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <h1>Kafka Advanced: Microservices</h1>
    <p>Apache Kafka is a powerful tool for building scalable and reliable microservices architectures. It enables microservices to communicate asynchronously, handle large volumes of data, and maintain high availability.</p>

    <h2>1. Kafka and Microservices</h2>
    <p>Microservices architecture breaks down an application into smaller, loosely coupled services that communicate over a network. Kafka plays a crucial role in microservices by providing:</p>
    <ul>
        <li><strong>Decoupling:</strong> Services can interact with each other through Kafka topics, reducing direct dependencies.</li>
        <li><strong>Scalability:</strong> Kafka handles large volumes of data and scales horizontally.</li>
        <li><strong>Fault Tolerance:</strong> Kafka ensures data durability and availability through replication and partitioning.</li>
    </ul>

    <h2>2. Implementing Microservices with Kafka</h2>
    <p>To build a microservices architecture with Kafka, you typically involve the following components:</p>
    <ul>
        <li><strong>Producers:</strong> Services that publish messages to Kafka topics.</li>
        <li><strong>Consumers:</strong> Services that subscribe to Kafka topics and process messages.</li>
        <li><strong>Topics:</strong> Logical channels where messages are published and consumed.</li>
        <li><strong>Streams:</strong> Kafka Streams API for real-time processing of streaming data.</li>
    </ul>

    <h3>2.1 Example: Microservices Communication</h3>
    <p>Consider two microservices, Service A and Service B, where Service A publishes user events and Service B processes these events.</p>

    <h4>Service A: Producer</h4>
    <pre><code>
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class ServiceAProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        // Publish a user event
        ProducerRecord<String, String> record = new ProducerRecord<>("user-events", "user1", "User event data");
        producer.send(record, (metadata, exception) -> {
            if (exception != null) {
                exception.printStackTrace();
            } else {
                System.out.println("Message sent to topic " + metadata.topic() + " partition " + metadata.partition());
            }
        });

        producer.close();
    }
}
    </code></pre>

    <h4>Service B: Consumer</h4>
    <pre><code>
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.Collections;
import java.util.Properties;

public class ServiceBConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "service-b-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("user-events"));

        while (true) {
            consumer.poll(100).forEach((ConsumerRecord<String, String> record) -> {
                System.out.printf("Consumed message with key %s and value %s%n", record.key(), record.value());
            });
        }
    }
}
    </code></pre>

    <h3>2.2 Handling Microservices Data</h3>
    <p>Kafka Streams can be used to process and enrich data in real-time across microservices:</p>
    <pre><code>
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.streams.kstream.ValueMapper;
import org.apache.kafka.common.serialization.Serdes;

import java.util.Properties;

public class MicroservicesStreamProcessor {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "microservices-stream-processor");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

        StreamsBuilder builder = new StreamsBuilder();

        // Stream processing logic
        KStream<String, String> inputStream = builder.stream("input-topic");
        KStream<String, String> enrichedStream = inputStream.mapValues(value -> "Processed: " + value);

        enrichedStream.to("output-topic", Produced.with(Serdes.String(), Serdes.String()));

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
    }
}
    </code></pre>

    <h2>3. Microservices Patterns and Best Practices</h2>
    <p>When using Kafka in a microservices architecture, consider the following patterns and best practices:</p>
    <ul>
        <li><strong>Event Sourcing:</strong> Store state changes as a sequence of events in Kafka topics.</li>
        <li><strong>CQRS (Command Query Responsibility Segregation):</strong> Separate command and query responsibilities to improve scalability and performance.</li>
        <li><strong>Idempotency:</strong> Ensure that operations are idempotent to handle potential duplicate messages.</li>
        <li><strong>Schema Evolution:</strong> Use schema registry to manage evolving data schemas and ensure compatibility.</li>
        <li><strong>Monitoring and Logging:</strong> Implement robust monitoring and logging to track the health and performance of your services.</li>
    </ul>

    <h2>4. Conclusion</h2>
    <p>Kafka is a powerful tool for building resilient and scalable microservices architectures. By leveraging Kafka's messaging and streaming capabilities, you can achieve effective decoupling, scalability, and fault tolerance in your microservices ecosystem.</p>
</body>
</html>
