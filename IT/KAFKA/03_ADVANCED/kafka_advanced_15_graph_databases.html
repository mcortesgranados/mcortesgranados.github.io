<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kafka Advanced: Graph Databases</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        header {
            background: #333;
            color: #fff;
            padding: 1rem 0;
            text-align: center;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #333;
        }
        pre {
            background: #eee;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            background: #f9f9f9;
            padding: 5px;
            border-radius: 3px;
        }
        .section {
            margin-bottom: 20px;
        }
        .example-code {
            background: #282c34;
            color: #61dafb;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <header>
        <h1>Kafka Advanced: Graph Databases</h1>
    </header>

    <div class="container">
        <section class="section">
            <h2>Introduction to Graph Databases</h2>
            <p>
                Graph databases are designed to handle data whose relationships are as important as the data itself. They use graph structures with nodes, edges, and properties to represent and store data. Graph databases are ideal for scenarios requiring complex queries on interconnected data, such as social networks, recommendation engines, and fraud detection systems.
            </p>
            <p>
                Popular graph databases include Neo4j, Amazon Neptune, and ArangoDB. These databases excel in scenarios where relationships between data points need to be traversed efficiently.
            </p>
        </section>

        <section class="section">
            <h2>Use Cases for Graph Databases</h2>
            <ul>
                <li><strong>Social Networks:</strong> Modeling relationships between users, friends, posts, and interactions.</li>
                <li><strong>Recommendation Engines:</strong> Suggesting products, services, or content based on user behavior and preferences.</li>
                <li><strong>Fraud Detection:</strong> Identifying suspicious activities by analyzing connections and patterns in transaction data.</li>
                <li><strong>Network Analysis:</strong> Managing and analyzing complex networks such as telecom networks or IT infrastructure.</li>
            </ul>
        </section>

        <section class="section">
            <h2>Integrating Kafka with Graph Databases</h2>
            <p>
                Kafka can be used to stream real-time data into graph databases, enabling applications to continuously update and query graph structures. Integration can be achieved using Kafka Connect, custom consumers, or stream processing frameworks.
            </p>

            <h3>Using Kafka Connect</h3>
            <p>
                Kafka Connect provides a framework for integrating Kafka with external systems, including graph databases. For example, you can use a Kafka Connect sink connector to write data from Kafka topics to a graph database.
            </p>
            <pre><code>
# Example Kafka Connect Configuration for Neo4j Sink
name=neo4j-sink
config={
    "connector.class": "com.neo4j.kafka.connect.Neo4jSinkConnector",
    "topics": "my-topic",
    "neo4j.server.uri": "bolt://localhost:7687",
    "neo4j.authentication.basic.username": "neo4j",
    "neo4j.authentication.basic.password": "password",
    "neo4j.node.label": "MyLabel",
    "neo4j.node.id": "id",
    "neo4j.node.fields": "field1,field2"
}
            </code></pre>

            <h3>Custom Kafka Consumers</h3>
            <p>
                For more customized processing, you can build a Kafka consumer that reads from a Kafka topic and writes to a graph database. This approach provides flexibility for complex transformations or integrations.
            </p>
            <pre><code>
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.neo4j.driver.AuthTokens;
import org.neo4j.driver.Driver;
import org.neo4j.driver.GraphDatabase;
import org.neo4j.driver.Session;
import org.neo4j.driver.Transaction;

import java.util.Collections;
import java.util.Properties;

public class KafkaToNeo4jConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("my-topic"));

        Driver driver = GraphDatabase.driver("bolt://localhost:7687", AuthTokens.basic("neo4j", "password"));
        try (Session session = driver.session()) {
            while (true) {
                for (ConsumerRecord<String, String> record : consumer.poll(100)) {
                    try (Transaction tx = session.beginTransaction()) {
                        tx.run("CREATE (n:MyLabel {id: $id, field1: $field1, field2: $field2})",
                                Map.of("id", record.key(), "field1", record.value(), "field2", "additional-data"));
                        tx.commit();
                    }
                }
                consumer.commitSync();
            }
        } finally {
            driver.close();
        }
    }
}
            </code></pre>

            <h3>Using Stream Processing Frameworks</h3>
            <p>
                Frameworks like Kafka Streams or Apache Flink can be used to process and enrich data in real-time before writing to a graph database. This allows for complex data transformations and aggregations.
            </p>
            <pre><code>
# Example of using Kafka Streams to process data
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.common.serialization.Serdes;
import java.util.Properties;

public class KafkaStreamsExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> sourceStream = builder.stream("input-topic");
        KStream<String, String> transformedStream = sourceStream.mapValues(value -> value.toUpperCase());
        transformedStream.to("output-topic", Produced.with(Serdes.String(), Serdes.String()));

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
    }
}
            </code></pre>
        </section>

        <section class="section">
            <h2>Conclusion</h2>
            <p>
                Integrating Kafka with graph databases allows for real-time data processing and enrichment, enabling powerful analytics and insights from interconnected data. Whether using Kafka Connect, custom consumers, or stream processing frameworks, this integration can enhance the capabilities of both Kafka and graph databases in handling complex data scenarios.
            </p>
        </section>
    </div>
</body>
</html>
