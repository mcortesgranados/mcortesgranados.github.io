<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kafka Intermediate: Kafka Connectors for Different Databases</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
        }
        h1 {
            color: #2c3e50;
        }
        h2 {
            color: #34495e;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 6px;
            overflow-x: auto;
        }
    </style>
</head>
<body>

    <h1>Kafka Intermediate: Kafka Connectors for Different Databases</h1>

    <p>Kafka Connect provides a powerful framework for integrating Kafka with various databases. Through specialized connectors, Kafka Connect allows you to stream data from databases into Kafka topics and from Kafka topics into databases.</p>

    <p>This guide will cover some common connectors used for different databases and provide examples for their configuration.</p>

    <h2>1. JDBC Source Connector</h2>
    <p>The JDBC Source Connector allows Kafka to read data from relational databases like MySQL, PostgreSQL, and Oracle. It is one of the most commonly used connectors due to its flexibility in handling a variety of databases that support JDBC.</p>

    <h3>Example: JDBC Source Connector Configuration</h3>
    <pre>
<code>
{
  "name": "jdbc-source-connector",
  "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
  "tasks.max": "1",
  "connection.url": "jdbc:mysql://localhost:3306/mydb",
  "connection.user": "dbuser",
  "connection.password": "dbpassword",
  "table.whitelist": "my_table",
  "mode": "incrementing",
  "incrementing.column.name": "id",
  "topic.prefix": "jdbc-",
  "poll.interval.ms": "10000",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter"
}
</code>
    </pre>

    <p>This configuration streams data from a MySQL database into Kafka by polling the specified table every 10 seconds. It tracks the data using an incrementing ID column.</p>

    <h2>2. MongoDB Source Connector</h2>
    <p>The MongoDB Source Connector allows Kafka to ingest data from MongoDB collections. It supports capturing new and updated records in MongoDB and streaming them into Kafka topics.</p>

    <h3>Example: MongoDB Source Connector Configuration</h3>
    <pre>
<code>
{
  "name": "mongodb-source-connector",
  "connector.class": "com.mongodb.kafka.connect.MongoSourceConnector",
  "tasks.max": "1",
  "connection.uri": "mongodb://localhost:27017",
  "database": "mydatabase",
  "collection": "mycollection",
  "topic.prefix": "mongo-",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter"
}
</code>
    </pre>

    <p>This configuration streams data from a MongoDB collection into Kafka topics, capturing the inserts and updates from the <code>mycollection</code> in the <code>mydatabase</code> database.</p>

    <h2>3. Cassandra Source Connector</h2>
    <p>The Cassandra Source Connector enables Kafka to integrate with Apache Cassandra, a NoSQL database. It allows streaming data from Cassandra tables into Kafka topics in real time.</p>

    <h3>Example: Cassandra Source Connector Configuration</h3>
    <pre>
<code>
{
  "name": "cassandra-source-connector",
  "connector.class": "com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector",
  "tasks.max": "1",
  "contact.points": "127.0.0.1",
  "port": "9042",
  "key.space": "mykeyspace",
  "table": "mytable",
  "poll.interval.ms": "5000",
  "topic.prefix": "cassandra-",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter"
}
</code>
    </pre>

    <p>This configuration streams data from a Cassandra table into Kafka topics, polling the table every 5 seconds to capture changes.</p>

    <h2>4. PostgreSQL Source Connector (Debezium)</h2>
    <p>The Debezium PostgreSQL Source Connector allows Kafka to capture changes in PostgreSQL databases using the logical replication feature. It streams change data (inserts, updates, and deletes) into Kafka topics.</p>

    <h3>Example: PostgreSQL Source Connector Configuration</h3>
    <pre>
<code>
{
  "name": "postgresql-source-connector",
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "tasks.max": "1",
  "database.hostname": "localhost",
  "database.port": "5432",
  "database.user": "postgres",
  "database.password": "password",
  "database.dbname": "mydatabase",
  "database.server.name": "dbserver1",
  "plugin.name": "pgoutput",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter",
  "table.whitelist": "public.mytable"
}
</code>
    </pre>

    <p>Using Debezium, this configuration captures changes from the PostgreSQL database and streams them into Kafka topics. It leverages the logical replication feature to monitor table changes.</p>

    <h2>5. Elasticsearch Sink Connector</h2>
    <p>The Elasticsearch Sink Connector allows you to write data from Kafka topics to an Elasticsearch index, enabling real-time search and analysis of data. It is useful for search applications and log aggregation.</p>

    <h3>Example: Elasticsearch Sink Connector Configuration</h3>
    <pre>
<code>
{
  "name": "elasticsearch-sink-connector",
  "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
  "tasks.max": "1",
  "topics": "mytopic",
  "connection.url": "http://localhost:9200",
  "type.name": "_doc",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter",
  "key.ignore": "true",
  "schema.ignore": "true"
}
</code>
    </pre>

    <p>This configuration writes data from the <code>mytopic</code> Kafka topic into an Elasticsearch index. The <code>key.ignore</code> and <code>schema.ignore</code> properties ensure that the Kafka message keys and schemas are ignored when indexing data.</p>

    <h2>6. Oracle Source Connector</h2>
    <p>The Oracle Source Connector allows Kafka to stream data from Oracle databases into Kafka topics. It can capture changes in real-time using Oracleâ€™s change data capture (CDC) features.</p>

    <h3>Example: Oracle Source Connector Configuration</h3>
    <pre>
<code>
{
  "name": "oracle-source-connector",
  "connector.class": "io.confluent.connect.oracle.OracleSourceConnector",
  "tasks.max": "1",
  "connection.url": "jdbc:oracle:thin:@localhost:1521:orcl",
  "connection.user": "oracleuser",
  "connection.password": "password",
  "table.whitelist": "my_table",
  "topic.prefix": "oracle-",
  "key.converter": "org.apache.kafka.connect.json.JsonConverter",
  "value.converter": "org.apache.kafka.connect.json.JsonConverter"
}
</code>
    </pre>

    <p>This configuration streams data from an Oracle database table into Kafka topics, using Oracle CDC to capture changes in real-time.</p>

    <h2>Conclusion</h2>
    <p>Kafka Connect supports a wide variety of connectors for different databases, enabling seamless data integration with Kafka. Whether you are working with relational databases like MySQL and PostgreSQL or NoSQL databases like MongoDB and Cassandra, Kafka Connect provides connectors to meet your needs.</p>

    <p>By using these connectors, you can easily stream data into and out of Kafka, creating a powerful, scalable, and real-time data pipeline.</p>

</body>
</html>
