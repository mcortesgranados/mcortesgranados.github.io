<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kafka Intermediate: Advanced Consumer Configuration</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
    </style>
</head>
<body>
    <h1>Kafka Intermediate: Advanced Consumer Configuration</h1>
    <p>Advanced consumer configurations in Apache Kafka enable fine-tuning the performance and behavior of Kafka consumers. These configurations are essential for optimizing consumer performance, ensuring data consistency, and handling various consumption scenarios efficiently.</p>

    <h2>1. Consumer Configuration Parameters</h2>
    <p>Kafka consumers have several configuration parameters that affect their performance and behavior. Here are some key advanced configurations:</p>

    <h3>1.1. <code>group.id</code></h3>
    <p>The <code>group.id</code> parameter specifies the consumer group the consumer belongs to. All consumers in the same group share the same group ID and work together to consume messages from Kafka topics:</p>
    <pre><code>
# Example configuration
props.put("group.id", "my-consumer-group");
    </code></pre>

    <h3>1.2. <code>auto.offset.reset</code></h3>
    <p>The <code>auto.offset.reset</code> parameter determines the behavior when there are no initial offsets or the current offsets are out of range. Possible values are:</p>
    <ul>
        <li><strong>earliest:</strong> Start reading from the earliest offset.</li>
        <li><strong>latest:</strong> Start reading from the latest offset.</li>
        <li><strong>none:</strong> Throw an exception if no previous offset is found.</li>
    </ul>
    <pre><code>
# Example configuration
props.put("auto.offset.reset", "earliest");
    </code></pre>

    <h3>1.3. <code>enable.auto.commit</code></h3>
    <p>The <code>enable.auto.commit</code> parameter controls whether offsets are automatically committed to Kafka:</p>
    <pre><code>
# Example configuration
props.put("enable.auto.commit", "true");
    </code></pre>

    <h3>1.4. <code>auto.commit.interval.ms</code></h3>
    <p>The <code>auto.commit.interval.ms</code> parameter specifies the frequency with which the offset is committed if <code>enable.auto.commit</code> is set to <code>true</code>:</p>
    <pre><code>
# Example configuration
props.put("auto.commit.interval.ms", "5000");
    </code></pre>

    <h3>1.5. <code>fetch.min.bytes</code></h3>
    <p>The <code>fetch.min.bytes</code> parameter sets the minimum amount of data the server should send in response to a fetch request. This setting can help improve efficiency by reducing the number of fetch requests:</p>
    <pre><code>
# Example configuration
props.put("fetch.min.bytes", "1024");
    </code></pre>

    <h3>1.6. <code>max.poll.records</code></h3>
    <p>The <code>max.poll.records</code> parameter specifies the maximum number of records returned in a single call to <code>poll()</code>:</p>
    <pre><code>
# Example configuration
props.put("max.poll.records", "100");
    </code></pre>

    <h2>2. Implementing Advanced Consumer Configuration in Java</h2>
    <p>Here is an example of how to configure a Kafka consumer with advanced settings in Java:</p>
    <pre><code>
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.Properties;
import java.util.Arrays;

public class AdvancedConsumerConfigExample {
    public static void main(String[] args) {
        // Step 1: Set up properties for the Kafka consumer
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-consumer-group");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "5000");
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, "1024");
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "100");
        
        // Step 2: Create a Kafka consumer with the specified properties
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        // Step 3: Subscribe to topics
        consumer.subscribe(Arrays.asList("my-topic"));
        
        // Step 4: Poll records from the topic
        while (true) {
            consumer.poll(100).forEach(record -> {
                System.out.printf("Received record with key %s and value %s%n", record.key(), record.value());
            });
        }
    }
}
    </code></pre>

    <h2>3. Key Considerations</h2>
    <ul>
        <li><strong>Consumer Group:</strong> Ensure all consumers in a group have the same <code>group.id</code> to balance the load and ensure message processing.</li>
        <li><strong>Offset Management:</strong> Choose between automatic and manual offset management based on your applicationâ€™s need for control and fault tolerance.</li>
        <li><strong>Performance Tuning:</strong> Configure <code>fetch.min.bytes</code> and <code>max.poll.records</code> based on your application's throughput and latency requirements.</li>
    </ul>

    <h2>4. Conclusion</h2>
    <p>Advanced consumer configurations in Kafka allow you to optimize consumer performance, handle offsets efficiently, and manage data consumption in a flexible manner. By tuning settings like auto-offset management, fetch sizes, and polling behavior, you can enhance the efficiency and reliability of your Kafka consumers.</p>
</body>
</html>
