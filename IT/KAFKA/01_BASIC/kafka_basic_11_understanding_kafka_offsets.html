<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Kafka Offsets</title>
    <style>
        body {
            font-family: Arial, Calibri, sans-serif;
            line-height: 1.6;
        }
        h1, h2 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
        code {
            font-family: Consolas, "Courier New", monospace;
        }
        .diagram {
            text-align: center;
            margin: 20px 0;
        }
        .diagram img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <h1>Understanding Kafka Offsets</h1>
    <p>This guide explains Kafka offsets, which are crucial for tracking the position of records in a Kafka topic. Proper understanding of offsets is essential for managing message consumption and ensuring data integrity.</p>

    <h2>1. What is a Kafka Offset?</h2>
    <p>A Kafka offset is a unique identifier for each record within a partition of a Kafka topic. It represents the position of a record in the log. Offsets are used by consumers to track and manage the records they have read from a topic.</p>

    <ul>
        <li><strong>Sequential Number:</strong> Offsets are sequential numbers, starting from 0 for the first record in a partition.</li>
        <li><strong>Partition-Specific:</strong> Each partition has its own set of offsets. The same offset value can appear in different partitions.</li>
        <li><strong>Commit:</strong> Consumers commit offsets to Kafka or manage them manually. This commit allows consumers to resume reading from the last processed record in case of failures or restarts.</li>
    </ul>

    <h2>2. How Offsets Work</h2>
    <p>Offsets help Kafka consumers keep track of which records have been processed and which are yet to be consumed. There are two primary ways to manage offsets:</p>

    <ul>
        <li><strong>Automatic Offset Management:</strong> Kafka can automatically commit offsets at regular intervals. This is controlled by the `enable.auto.commit` configuration property.</li>
        <li><strong>Manual Offset Management:</strong> Consumers can manually commit offsets using the `commitSync()` or `commitAsync()` methods, providing more control over offset management.</li>
    </ul>

    <h2>3. Example: Manual Offset Management</h2>
    <p>Below is a Java example demonstrating how to manage offsets manually using Kafka's Consumer API. This example shows how to commit offsets after processing records to ensure that the consumer resumes from the correct position on restart.</p>

    <pre><code>
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

public class ManualOffsetManagementExample {
    public static void main(String[] args) {
        // Consumer configuration
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "manual-offset-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); // Disable auto commit
        
        // Create KafkaConsumer
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        // Subscribe to topic
        consumer.subscribe(Collections.singletonList("my-topic"));
        
        // Poll for new records
        while (true) {
            consumer.poll(100).forEach(record -> {
                System.out.printf("Received record: key=%s value=%s partition=%d offset=%d%n",
                                  record.key(), record.value(), record.partition(), record.offset());

                // Manually commit offsets after processing
                Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1));
                consumer.commitSync(offsets);
            });
        }
    }
}
    </code></pre>

    <h2>4. Kafka Offsets Diagram</h2>
    <p>The following diagram illustrates how offsets are used to track records in a Kafka topic partition and how they are managed by consumers.</p>

    <div class="diagram">
        <img src="kafka-offsets-diagram.png" alt="Kafka Offsets Diagram">
        <p><em>Diagram: Kafka Offsets Tracking and Management</em></p>
    </div>

    <h2>5. Conclusion</h2>
    <p>Understanding Kafka offsets is crucial for effective data consumption and processing. Proper offset management ensures that consumers can accurately track their progress and handle data recovery in case of failures.</p>
</body>
</html>
