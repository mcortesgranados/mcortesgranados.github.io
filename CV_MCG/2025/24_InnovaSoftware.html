<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Innova Software CV_MCG 2025</title>
<style type="text/css">
<!--
.style1 {font-family: Arial, Helvetica, sans-serif}
-->
</style>
</head>

<body>
<p class="style1"><strong>Company Experience 23</strong><br />
  Company  name: <strong>InnovaSoftware</strong><br />
  Role:  Systems Engineer / Java Software Developer<br />
  Duration:  04/2024 – Present</p>
<p class="style1"><strong>Key Responsibilities &amp; Achievements</strong></p>
<ul type="disc" class="style1">
  <li><strong>ManyChat       Integration</strong> </li>
  <ul type="circle">
    <li>Developed        and deployed <strong>chatbot automation workflows</strong> using <strong>ManyChat</strong> for lead generation and customer engagement.</li>
    <li>Integrated        ManyChat with <strong>backend APIs (Java Spring Boot / Python Flask)</strong> to        handle real-time customer interactions.</li>
    <li>Implemented <strong>webhooks and API endpoints</strong> to capture and store user responses in        a centralized database.</li>
  </ul>
  <li><strong>HubSpot       Integration</strong> </li>
  <ul type="circle">
    <li>Designed        and implemented <strong>HubSpot CRM integrations</strong> using <strong>Java and Python</strong>,        ensuring seamless data synchronization between HubSpot and internal        systems.</li>
    <li>Developed <strong>custom        API connectors</strong> to <strong>retrieve, update, and manage contacts, deals,        and engagements</strong> within HubSpot.</li>
    <li>Optimized        API requests using <strong>pagination and batch processing</strong>, improving        performance and reducing API quota consumption.</li>
  </ul>
  <li><strong>Integration       with Alegra &amp; Siigo</strong> (Accounting &amp; Invoicing Systems)</li>
  <ul type="circle">
    <li>Built <strong>automated        invoice generation and payment processing workflows</strong> by integrating        with <strong>Alegra and Siigo APIs</strong>.</li>
    <li>Developed        RESTful microservices in <strong>Spring Boot and FastAPI</strong>, ensuring <strong>secure        data exchange</strong> between financial systems and internal applications.</li>
    <li>Implemented <strong>OAuth authentication and API token management</strong> for secure access to        accounting platforms.</li>
  </ul>
  <li><strong>Full Stack       Development</strong> </li>
  <ul type="circle">
    <li>Developed        and maintained <strong>React.js and Angular-based front-end applications</strong>,        integrating seamlessly with backend services.</li>
    <li>Designed        and optimized <strong>RESTful APIs</strong> using <strong>Spring Boot, Django, and        FastAPI</strong>, ensuring high performance and scalability.</li>
    <li>Worked with <strong>PostgreSQL, MySQL, and MongoDB</strong> for efficient data storage and        retrieval.</li>
    <li>Deployed        applications on <strong>AWS and Azure</strong>, leveraging <strong>Lambda, EC2, and        Docker</strong> for cloud-based deployment.</li>
  </ul>
</ul>
<p class="style1"><strong>Technologies &amp; Tools Used:</strong></p>
<ul type="disc" class="style1">
  <li><strong>Backend:</strong> Java       (Spring Boot), Python (FastAPI, Flask, Django)</li>
  <li><strong>Frontend:</strong> React.js,       Angular, NextJS, Node JS</li>
  <li><strong>Databases:</strong> PostgreSQL,       MySQL, MongoDB</li>
  <li><strong>Cloud &amp;       DevOps:</strong> AWS, Azure, Docker, Kubernetes</li>
  <li><strong>APIs &amp;       Integrations:</strong> ManyChat, HubSpot API, Alegra API, Siigo API</li>
</ul>
<p class="style1">Achievement/Goals:</p>
<p class="style1">He  has carried out custom developments in the .NET platform. Development in  ASP.NET and C# in web applications. Good knowledge of the SQL Server  Integration Service platform for data integration. He has also carried out  developments in the following Java frameworks:</p>
<p class="style1">1  – Mockito Library: Very good knowledge about the implementation of unit tests  through the JUnit framework, and the simulation of mockups through the mockito  library. Implementation of JUnit unit tests for classes that implement DAO  (Database Connection) pattern and REST services.<br />
  2  - Implementation of REST services in Spring (Use of Spring @RestController and  @Endpoint technologies)<br />
  3  - Knowledge of the EclEmma Java Code Coverage 3.0.0 component and SonarQube,  which allows visualizing the coverage of tests implemented in JUnit in the  various classes of the project. Useful for code quality certification issues.<br />
  4  – Custom development: Development of an automatic analyzer of Logs generated  from the Oracle Service Bus 12c administration console. It allows to detect  errors that affect the server more quickly.<br />
  5  – Various developments and consultancies related to programming in Visual  Studio .NET<br />
  6  – Personalized OSB development: Advice and support in a development in Java JEE  that receives as input parameter the log file generated from the SOA/BUS ESP  Business Console and generates analytical reports which allow the different  types of errors that can be visualized by component. can be presented in the  Oracle Service Bus at the Productive Level.<br />
  7  – Expert in CI/CD Pipeline implementation: Expert in configuring a CI/CD  Pipeline through the parameterization of the following tools/performing the  following tasks: 7.1) Creation of github forks 7.2) Automated tests in Graddle  7.3) Integration continues with Jenkins 7.4) Continuous delivery through  creation of pipelines in Jenkins, pipeline stages and steps, deployment through  pipelines and deployment 7.5) Parameterization of jenskins pipelines in a  dockerized application (Docker) 7.6) Orchestration through clustering in  Kubernetes 7.6) Monitoring through Prometheous and Grafana 7.7) Creation of  Liveness probes in Kubernetes 7.7) Horizontal Pod Autoscalers with Kubernetes  7.8) Implementation of a Canary test in Kubernetes 7.9)<br />
  8  – Use of Apache Airflow: Integrations/flow design has been carried out with  Apache airflow for 8.1) construction and management of data pipelines and ETL  processes, extracting data from various sources, carrying out transactions, and  uploading data to the systems destination, and data flow orchestration 8.2)  Integration with data warehouses 8.3) AI machine learning data flows 8.4)  Report generation and business intelligence 8.5) DevOps infrastructure  automation 8.6) Event-managed architectures 8.7) Data processing<br />
  9  – ETL Development: ETL development with Microsoft SQL Server Integration  Services (SSIS), IBM InfoSphere DataStage, Pentaho Data Integration and Oracle  Business Intelligence.<br />
  10  – Cybersecurity Tools: Splunk<br />
  11  – Data Management Tool/Data Science/Data Engineer: Development of applications  for data visualization and statistics. Deployment of Jupiter Hub on Google  Cloud.<br />
  12  – Use of Hadoop and Sqoop for intelligent storage: Bigdata storage strategies  in hadoop taking advantage of its features such as HDFS, MapReduce, YARN (Yet  another resource negotiator). Use of sqoop for purposes of maintaining  consistency, ensuring efficient use of resources, in order to load data from  Hadoop to relational databases and vice versa, through the kerberos network  authentication protocol.<br />
  13  – Snowflake: Regarding my experience with Snowflake, I spearheaded advanced  data analytics initiatives using Snowflake, a cloud-based data warehousing  platform in freelancing work. Leveraging Snowflake's powerful features, I  architected complex data models and optimized query performance to handle  large-scale datasets efficiently. One notable project involved implementing  data sharing across multiple organizations, requiring intricate access control  mechanisms and ensuring data privacy compliance. Additionally, I orchestrated  the migration of on-premises data warehouses to Snowflake, meticulously  designing migration strategies and overseeing seamless data transfer processes.  Through the use of Snowflake's semi-structured data capabilities, I developed  innovative solutions for parsing and analyzing unstructured data sources,  unlocking valuable insights for decision-makers. Furthermore, I collaborated  with cross-functional teams to establish best practices for data governance,  including data lineage tracking and metadata management within Snowflake. By  harnessing Snowflake's elasticity and scalability, I orchestrated automated  data pipelines and real-time data processing workflows, enabling  near-instantaneous data insights for business stakeholders. Overall, my  expertise in Snowflake empowered the organization to harness the full potential  of its data assets, driving strategic decision-making and fostering a  data-driven culture.</p>
<p class="style1">Experience  with Mobile Applications Development: The development of applications for  billing generation, inventory control and monitoring, and sales management in  general was carried out through an app, for Android devices with the Android  Studio tool and through the integration of Flutter with IntelliJ through the  Dart plugin, using the Flutter SDK.</p>
<p class="style1">iPaaS  experience: I possess extensive iPaaS (Integration Platform as a Service)  experience with AWS and Azure. In AWS, I've effectively utilized services such  as AWS Lambda, known for its serverless computing capabilities, enabling me to  execute code without provisioning or managing servers. Additionally, AWS Step  Functions have been instrumental in orchestrating complex workflows, providing  a reliable way to coordinate multiple AWS services. AWS API Gateway has allowed  me to build, deploy, and manage APIs at scale, while integrating with various  backend systems securely. Furthermore, I've leveraged AWS EventBridge for  event-driven architectures, facilitating decoupled and scalable application  designs. AWS SQS (Simple Queue Service) has been pivotal in enabling reliable  and scalable messaging between distributed systems.</p>
<p class="style1">On  the Azure platform, I've proficiently employed Azure Logic Apps, a powerful  workflow automation tool that integrates seamlessly with over 200 connectors,  allowing for rapid development of integration solutions. Azure Functions have  enabled me to build event-driven, serverless applications with ease, providing  scalability and cost-effectiveness. Azure API Management has been instrumental  in ensuring secure and scalable API integrations, offering features such as  rate limiting, authentication, and monitoring. Moreover, Azure Event Grid has  facilitated event routing and filtering, enabling real-time reaction to changes  and events within the system. Additionally, I've utilized Azure Service Bus for  reliable messaging between applications and services, ensuring guaranteed  message delivery and FIFO (First-In-First-Out) ordering.</p>
<p class="style1">12  – Python Development: 5 years of experience using Python Development</p>
<p class="style1">Convention:  Python libraries are in GREEN color, AWS Technologies are detailed in BLUE  color, relevant technologies application are detailed in PURPLE color</p>
<p class="style1">12.1.  Good knowledge of frameworks Django and Flasks.   Django: Knowledge about creation of App, Views, Models, templates,  Django data insertion, Django prepare template.  <br />
  12.2.  Implementation of Python applications as an alternative backend for Java.  Deployment of Django in AWS Elastic  Beanstalk.  Good knowledge of Flask:  Structure of projects through Blueprints, Flask SQLAlchemy for ORM database  support.  <br />
  12.3.  Use of python in AWS Infrastructre<br />
  12.3.1.  Use of   Python language in AWS Lambda<br />
  12.3.1.1.  Creation of API Gateway and Microservices processing requests and providing  dynamic responses.<br />
  12.3.1.2.  Process of data from S3, perform transformations, and loading of data into a  database.<br />
  12.3.1.3.  User of OpenCV for video real-time processing and detection of particular  objects in videos.<br />
  12.3.1.4.  File conversion from Word to PDF<br />
  12.3.1.5.  Implementation of chatbots that allow interaction with users, answer questions,  through Python’s NPL libraries like NLTK or spaCy.<br />
  12.3.1.6.  User of Lambda to run functions at specific Intervals using CloudWatch Events  for daily reports, backups and system maintenance.<br />
  12.3.1.7.  User of Python Library BeaitifulSoup in order to parse HTML and retrieve  information Web Scraping.<br />
  12.3.1.8.  Implement of operations such as S3 bucket uploads, Dynamo Updates through the  carry on of event-development applications.<br />
  12.3.2.  Use of   Python language in another AWS technologies<br />
  12.3.2.1.  Very good knowledge of AWS CloudFormation SDK for Python boto3 for the  creation, deletion, and update of resources.<br />
  12.3.2.2.  Use of python libraries such as pandas and NumPy, in order to perform data  analysis and data processing on AWS, over technologies such as Amazon S3,  Amazon Redshift, Amazon Athena and Amazon EMR.   Implementations carrying on the following : data transformation and  cleaning, data aggregation and summarization, data enrichment and augmentation,  text processing and natural language processing (NLP) using libraries such as  NLTK, spaCY and TextBlob in order to apply sentiment analysis, entity  recognition, text classification.  Use of  machine learning libraries such as scikit-learn, TensorFlow and PyTorch in order  to perform build, train and deploy machine learning models for tasks like  regression, classification and clustering.<br />
  12.3.2.3.  Use of python in order to process and analysis of streaming data as it arrives,  use of python implementations for decision making, action triggering according  to the response of certain patterns. <br />
  12.3.2.4.  Use of Panda, numPY and statsmodels for time series analysis tasks, anomaly  detection and trend identification.<br />
  12.3.2.5.  Use of GeoPanda for the implementation of Python application that apply  geospatial data, in order to perform spatial analysies and the creation of  visualizations of geographic information.<br />
  12.3.2.6.  Use of the following libraries: Matplotlib, Seaborn and Plotly in order to  enable data visualization in order to convey insights effectively.<br />
  12.3.2.7.  Use of the following libraries in order to do implementations related with  Machine Learning and AI in Python applied in AWS: sciki-learn, TensorFlow,  PyTorch and XGBoost inside AWS SageMaker in order to build, train and deploy  machine learning models at scale.<br />
  12.3.2.8.  Use of Python in order to define workflows, event rules and the message  processing in serverless architecture through AWS Services such as AWS Step  Functions, Amazon EventBridge and Amazon SQS.<br />
  12.3.2.9.  Development of scripts using Python in order to enable to implement deployment  pipelines through AWS services such as AWS CodePipeline and AWS CodeBuild.<br />
  12.3.2.10.  Use of Python scripts in order to enable the  assurance of security  checks, compliance  audits and vulnerability assessments over AWS resources.<br />
  12.3.  Use of python in Azure<br />
  12.3.1.  Use of python for Data Processing, data analysis, machine learning and big data  through PySpark in Azure DataBricks.<br />
  12.3.2.  Use of python scripts in Azure Data pipelines in order to process and transform  data.<br />
  12.3.3.  Use of python in order to process big data using Hadoop and Spark clusters.<br />
  12.3.4.  User of python libraries such as scikit-learn, TensorFlow and PyTorch in order  to implement machine learning models in Azure ML.<br />
  12.3.5.  User of python in order to implement serverless functions and allow the  triggering of this functions based on events or HTTP requests.<br />
  12.3.6.  Use of Azure Kubernates Service (AKS) in order to allow the containerization  and deployment of Python applications to manage the orchestration of  containers.<br />
  12.3.7.  Use of python language in order to interact with Azure API management for the  creation and APIs management, Azure Monitor and Azure Automation for the  automation of management tasks and the monitoring of resources, Azure Service  Bus in order to deploy the implementation for sending and receiving messages  through Azure Service Bus queues and topics.</p>
</body>
</html>
